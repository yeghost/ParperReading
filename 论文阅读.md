# XORing Elephants: Novel Erasure Codes for Big Data

## 0. ABSTRACT

RS基础上做了改进，通过增加存储冗余，优化性能

## 1. INTRODUCTION

介绍了多副本，纠删码。

纠删码的修复主要问题是，带宽开销，假如是（10，4），修复一块需要10倍块大小的带宽

提出了LRC的概念

## 1.1. Importance of Repair

分析了Facebook的相关数据得出降低网络带宽代价是重要的结论

efficiently repairable重要的原因

1. degraded reads

   > 很多瞬时错误并不会丢失永久性数据，但是会造成数据不可用，这个时候读编码的条块会读降级。
   >
   > 这个时候可以通过修复过程重建数据块，但是目的不是容错，而是为了更高的数据可用性。重建的块不必写入磁盘
   >
   > 所以efficient and fast repair 可以提高数据可用性

2. efficient node decommissioning

   >Hadoop可以让故障节点退役
   >
   >Functional data必须在退役之前从节点中复制出来，这是一个复杂且耗时的过程
   >
   >Fast repairs可以把节点退役视为定期修复，并且重新创建块，不会产生非常大的网络流量

3. repair influences the performance of other concurrent MapReduce jobs

   > 因为修复需要占用一定的网络带宽，与数据中心的网络带宽相比，存储空间的增长速度不成比例地快。所以这个问题会越来越严重，所以local repairs显得越发重要

4. local repair would be a key in facilitating geographically distributed file systems across data centers

   >RS在跨地理的程度上是不可行的 因为high bandwidth requirements across wide area networks
   >
   >local repair是可行的

复制可以很好的处理上面的问题，但是存储开销较大。MDS开销小，但是会遇到上面的问题。

本文可以看作是牺牲了一些存储效率，来达到其他指标

## 2. THEORETICAL CONTRIBUTIONS

MDS在通信和存储领域应用非常广泛

MDS是最低恢复冗余

两个定义

1. Minimum Code Distance
2. Block Locality

locality和good distance是矛盾的

LRC(k, n−k, r)

### 2.1. LRC implemented in Xorbas

Ci的选择有要求 线性无关，不能为0

设计了一个随机算法和确定算法，可以生成系数

有个优化S3不用存储，构造出来S1+S2+S3=0

系数的构造

## 3. SYSTEM DESCRIPTION

在HDFS-RAID上实现

> RaidNode 
>
> > 负责创建和维护校验块
>
> BlockFixer
>
> > 用来修复块
>
> ErasureCode
>
> > 实现编码和解码功能，上面两个组件都依赖于他

HDFS-Xorbas 在HDFS-RAID基础上增加了LRC

### 3.1. HDFS-Xorbas

#### 3.1.1. 编码

RaidNode 将一个文件分成10块，然后编码出4块。可能一个文件可能不够10块，默认剩下的填充为0，依旧是10块

![image-20211009150941798](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211009150941798.png)

LRC会额外计算两个块，如上图所示

#### 3.1.1.2 解码和修复

RaidNode

> light-decoder
>
> > 针对于每个条带单个块的错误
>
> heavy-decoder
>
> > light-decoder失败的时候使用

BlockFixer

> 检测到块失败，决定LRC恢复需要的5个块
>
>  light-decoding尝试恢复
>
> 如果出现multiple failures，可能没有需要的5个块， light-decoder失败， heavy-decoder启动
>
> heavy-decoder跟RS恢复过程一样，将结果发送和存储到数据节点

##  4.RELIABILITY ANALYSIS

 mean-time to data loss (MTTDL) 可靠性分析的依据

> 可以容忍的故障数
>
> 修复的速度
>
> 弹性增加和修复时间减小，MTTDL也会增加

结果LRC，RS比复制高了很多，但是复制的数据可用性比LRC和RS强

## 5. EVALUATION

两种环境性能

Amazon’s Elastic Compute Cloud

 a test cluster in Facebook

### 5.1 评价指标

HDFS Bytes Read

> 对应于为修复而启动的作业所读取的总数据量
>
> collected from the statistics-reports of the jobs spawned following a failure event

Network Traffic

> the total amount of data communicated from nodes in the cluster
>
> 单位为GB
>
> 用下面这个工具来监测
>
> Amazon’s A WS Cloudwatch monitoring tools

Repair Duration

>  the time interval between the starting time of the first repair job and the ending time of the last repair job.

### 5.2 EC2

two Hadoop clusters

> HDFS-Xorbas
>
> HDFS-RS

Each cluster

>  51 instances of type m1.small
>
> 1 master  hosting Hadoop’s NameNode, JobTracker and RaidNode daemons
>
> 50  slave as DataNode and a TaskTracker daemon

file size 640 MB

block size 64MB

每个文件两个集群中分别生成14和16块

故障包括一个数据节点或者多个数据节点的终止

四个故障事件是单节点错误，两个三节点错误，两个两节点错误

文件数量（20，100，200）

#### 5.2.1 HDFS Bytes Read

HDFS-Xorbas 比HDFS-RS好41%-52%

读的平均块数从11.5降到5.8

#### 5.2.2 Network Traffic

网络流量和读取的字节数基本上一致，二倍的关系

#### 5.2.3 Repair Time

Xorbas比HDFS-RS快25%到45%

实验里面带宽没满，实际环境中带宽可能跑满，时间表现可能更好

#### 5.2.4 Repair under Workload

为了演示修复性能对集群负载的影响。

创建了两个集群

每个集群15个从节点

块故障时不可用，LRC相比RS延迟小

### 5.3 Facebook’s cluster

区别点在于利用的集群中现有的数据集

块大小为256MB

94%文件3块 剩下10块 平均3.4块

由于块大小比较小，Xorbas比HDFS-RS存储开销大了27%（最好应为13%）

## 6. RELATED WORK

functional repair

> 虽然块可以恢复，但是这个时候确实不可使用，需要其他k块来恢复

exact repair

使用更小网络代价来修复是有可能的

> low rate
>
> high rate

这部分涉及到的工作还挺多的，有时间可以看一看具体内容

## 7. CONCLUSIONS

LRC降低带宽开销2倍，增加存储开销14%

提出了想法，应用在宽条带上，RS在宽条带上不可行，因为带宽要求随着块大小增长



# StripeMerge: Efficient Wide-Stripe Generation for Large-Scale Erasure-Coded Storage

## 0.Abstract

极限存储-宽条带 带宽开销巨大

StripeMerge 宽条带生成机制

将窄条带变成宽条带，这个过程中是为了最小化宽条带生成带宽

本文工作证明了最优方案的存在，还有两个启发式算法，生成时间减小87.8%

## 1.INTRODUCTION

为了保证data durability提出复制，但是冗余开销太大

RS是一种替代方案，冗余开销很低，但是现在还在追求更低的存储开销

宽条带

> 极端的存储冗余
>
>  more on low-cost storage durability than high data access performance
>
> 重构的时候，带宽消耗很严重

应该根据年龄不同，做不同的参数化，分层处理

> 数据块在新写入时往往被访问得更频繁，但随着它们的年龄变得更少
>
> 随着数据的老化，窄条带被重新编码为宽条带

将窄条带重新编码为宽条带不可避免地会重新定位数据块并重新生成校验块，从而导致数据传输中巨大的带宽开销

主要观点

宽条带的生成发生在有大量节点和条带的大型存储系统

选择两个窄条带合并成一个

 StripeMerge

## 2.BACKGROUND ANDMOTIVATION

### A Erasure Coding

讲了纠删码的概念，MDS码

### B Wide-Stripe Generation

宽条带的概念

假设

> 宽条带用于很少被访问的冷数据，比如备份和归档数据[1]、[6]或二进制大对象(blob)，它们的访问频率随着时间的推移而下降

宽条带有较高的修复代价，随着k增大而增大

宽条带生成问题

>宽条带生成步骤（两个窄条带生成一个宽条带）
>
>>重新分配2k块，分布在不同的节点
>>
>>将两个窄条带的一些数据和校验块迁移到一些负责生成宽条带的新校验块的节点上，它们的奇偶校验块稍后分布在不同的节点上。
>
>数据块的重新分布，从窄条带重新生成校验块，存在明显的数据传输
>
>目标最小化带宽

### C Storage Scaling

增加新节点扩容，为了在现有和新增加的节点上重新分配擦除编码块，他们研究了如何将(k,m)条转换为(k+s,m)条，以最小化伸缩带宽(即伸缩期间传输的数据量)

NCScale的扩展方法

![image-20211014093039882](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014093039882.png)

此时宽条带生成带宽依旧不为0，宽条带与存储扩展有所不同的是它不需要增加新的节点，这种差异导致它有不同的解决方案。

### D Our Idea

Perfect merging

> 从当前存储的大量窄条带中，选择两条合适的窄条带，用来合并为宽条带，不需要生成带宽

例子如下

![image-20211014095141389](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014095141389.png)

b和a的区别在于c ，d在单独的节点，并不需要再迁移，这个地方不消耗带宽

图示的两个条带有这样的特点，用来消除宽条带生成带宽

> 数据块在不同的节点
>
> > 因为宽条带最后数据块分布在不同的节点，如果有在相同节点的，那么会导致数据块迁移到其他节点，有带宽开销
>
> 校验块有相同的编码系数，并驻留在相同的节点，这些可以用来生成新的校验块

例子如下

![image-20211014095936799](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014095936799.png)

![image-20211014095943051](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014095943051.png)

由上面两个式子可以看出来，新的校验块可以通过之前的校验块本地生成，不产生带宽消耗

![image-20211014100208111](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014100208111.png)

**Challenges**：Perfect merging不产生带宽开销，但是如何在系统中应用Perfect merging生成多个宽条带。并且Perfect merging跟窄条带的放置方式有关。大规模存储系统中，搜索所有的窄条带会消耗大量时间。

## 3.ANALYSIS

SectionIII-A 我们将宽条带生成问题转换为bipartite graph model

SectionIII-B 证明在给定足够多的窄条带的情况下，总是存在一个可以生成所有宽条带而不需要任何宽条带生成带宽的最优方案。

### A Bipartite Graph Model

n个节点，足够多的(k,m)narrow stripes，目标是选择所有符合Perfect merging的窄条带，合并成(2k,m)的宽条带，这样就不存在跨节点的生成带宽

（k,m）条带，随机分布在k+m节点上

![image-20211014102642933](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014102642933.png)

总共排列方案，假设是每个都不同。但是实际上我们合并的时候，数据块顺序无所谓（我们计算的时候只用了校验块），校验块的顺序是重要的，对计算产生影响（主要是前面的系数）

![image-20211014104058750](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014104058750.png)

这种被认为是同一种摆放方式。

![image-20211014104323104](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014104323104.png)

所以总共可能性如上，被称为complete chunk placement set。

![image-20211014104423710](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014104423710.png)

## 4.STRIPEMERGE



## 5.PERFORMANCEEVALUATION



## 6.RELATEDWORK

在repair和scaling roblem最小化带宽的研究

repair

>locally repairable codes (LRC)
>
>> 通过额外的存储减小修复I/O
>
>regenerating codes(再生码)
>
>> 通过额外的存储减小修复I/O
>
>repair-efficient techniques
>
>> lazy recovery通过谨慎地延迟立即修复操作来减少修复流量
>
>parallelizing and pipelining repair
>
>> 减少修复时间
>
> scheduling repair tasks in free timeslots
>
>> 适应工作负载的动态变化

scaling problem

> minimize the bandwidth
>
> >  under RAID-0, RAID-5 (i.e., single fault tolerance) , or RS codes
>
> code conversion
>
> > 研究可转换的代码结构，以最小化代码转换中的I/O

本文研究的问题其实和上面的都不一样，研究如何最小化带宽并且减少宽条带生成问题的计算开销。

宽条带问题的相关研究

> VAST
>
> > locally decodable codes提高宽条带的修复性能
>
> Haddock et 
>
> > general-purpose GPUs 提高宽条带的解码效率
>
> ECWide
>
> > combined locality
> >
> > 系统的解决了修复带宽问题，提出了有效的编码和更新方案

本文关注点在于如何生成宽条带

## 7.CONCLUSIONS

StripeMerge生成宽条带

本文转换为bipartite graph modeling，证明了最优方案的存在。提出了两个算法，在有限带宽下生成（完美情况时间复杂度比较大），生成性能比现有性能好。















