# XORing Elephants: Novel Erasure Codes for Big Data

## 0. ABSTRACT

RS基础上做了改进，通过增加存储冗余，优化性能

## 1. INTRODUCTION

介绍了多副本，纠删码。

纠删码的修复主要问题是，带宽开销，假如是（10，4），修复一块需要10倍块大小的带宽

提出了LRC的概念

## 1.1. Importance of Repair

分析了Facebook的相关数据得出降低网络带宽代价是重要的结论

efficiently repairable重要的原因

1. degraded reads

   > 很多瞬时错误并不会丢失永久性数据，但是会造成数据不可用，这个时候读编码的条块会读降级。
   >
   > 这个时候可以通过修复过程重建数据块，但是目的不是容错，而是为了更高的数据可用性。重建的块不必写入磁盘
   >
   > 所以efficient and fast repair 可以提高数据可用性

2. efficient node decommissioning

   >Hadoop可以让故障节点退役
   >
   >Functional data必须在退役之前从节点中复制出来，这是一个复杂且耗时的过程
   >
   >Fast repairs可以把节点退役视为定期修复，并且重新创建块，不会产生非常大的网络流量

3. repair influences the performance of other concurrent MapReduce jobs

   > 因为修复需要占用一定的网络带宽，与数据中心的网络带宽相比，存储空间的增长速度不成比例地快。所以这个问题会越来越严重，所以local repairs显得越发重要

4. local repair would be a key in facilitating geographically distributed file systems across data centers

   >RS在跨地理的程度上是不可行的 因为high bandwidth requirements across wide area networks
   >
   >local repair是可行的

复制可以很好的处理上面的问题，但是存储开销较大。MDS开销小，但是会遇到上面的问题。

本文可以看作是牺牲了一些存储效率，来达到其他指标

## 2. THEORETICAL CONTRIBUTIONS

MDS在通信和存储领域应用非常广泛

MDS是最低恢复冗余

两个定义

1. Minimum Code Distance
2. Block Locality

locality和good distance是矛盾的

LRC(k, n−k, r)

### 2.1. LRC implemented in Xorbas

Ci的选择有要求 线性无关，不能为0

设计了一个随机算法和确定算法，可以生成系数

有个优化S3不用存储，构造出来S1+S2+S3=0

系数的构造

## 3. SYSTEM DESCRIPTION

在HDFS-RAID上实现

> RaidNode 
>
> > 负责创建和维护校验块
>
> BlockFixer
>
> > 用来修复块
>
> ErasureCode
>
> > 实现编码和解码功能，上面两个组件都依赖于他

HDFS-Xorbas 在HDFS-RAID基础上增加了LRC

### 3.1. HDFS-Xorbas

#### 3.1.1. 编码

RaidNode 将一个文件分成10块，然后编码出4块。可能一个文件可能不够10块，默认剩下的填充为0，依旧是10块

![image-20211009150941798](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211009150941798.png)

LRC会额外计算两个块，如上图所示

#### 3.1.1.2 解码和修复

RaidNode

> light-decoder
>
> > 针对于每个条带单个块的错误
>
> heavy-decoder
>
> > light-decoder失败的时候使用

BlockFixer

> 检测到块失败，决定LRC恢复需要的5个块
>
>  light-decoding尝试恢复
>
> 如果出现multiple failures，可能没有需要的5个块， light-decoder失败， heavy-decoder启动
>
> heavy-decoder跟RS恢复过程一样，将结果发送和存储到数据节点

##  4.RELIABILITY ANALYSIS

 mean-time to data loss (MTTDL) 可靠性分析的依据

> 可以容忍的故障数
>
> 修复的速度
>
> 弹性增加和修复时间减小，MTTDL也会增加

结果LRC，RS比复制高了很多，但是复制的数据可用性比LRC和RS强

## 5. EVALUATION

两种环境性能

Amazon’s Elastic Compute Cloud

 a test cluster in Facebook

### 5.1 评价指标

HDFS Bytes Read

> 对应于为修复而启动的作业所读取的总数据量
>
> collected from the statistics-reports of the jobs spawned following a failure event

Network Traffic

> the total amount of data communicated from nodes in the cluster
>
> 单位为GB
>
> 用下面这个工具来监测
>
> Amazon’s A WS Cloudwatch monitoring tools

Repair Duration

>  the time interval between the starting time of the first repair job and the ending time of the last repair job.

### 5.2 EC2

two Hadoop clusters

> HDFS-Xorbas
>
> HDFS-RS

Each cluster

>  51 instances of type m1.small
>
> 1 master  hosting Hadoop’s NameNode, JobTracker and RaidNode daemons
>
> 50  slave as DataNode and a TaskTracker daemon

file size 640 MB

block size 64MB

每个文件两个集群中分别生成14和16块

故障包括一个数据节点或者多个数据节点的终止

四个故障事件是单节点错误，两个三节点错误，两个两节点错误

文件数量（20，100，200）

#### 5.2.1 HDFS Bytes Read

HDFS-Xorbas 比HDFS-RS好41%-52%

读的平均块数从11.5降到5.8

#### 5.2.2 Network Traffic

网络流量和读取的字节数基本上一致，二倍的关系

#### 5.2.3 Repair Time

Xorbas比HDFS-RS快25%到45%

实验里面带宽没满，实际环境中带宽可能跑满，时间表现可能更好

#### 5.2.4 Repair under Workload

为了演示修复性能对集群负载的影响。

创建了两个集群

每个集群15个从节点

块故障时不可用，LRC相比RS延迟小

### 5.3 Facebook’s cluster

区别点在于利用的集群中现有的数据集

块大小为256MB

94%文件3块 剩下10块 平均3.4块

由于块大小比较小，Xorbas比HDFS-RS存储开销大了27%（最好应为13%）

## 6. RELATED WORK

functional repair

> 虽然块可以恢复，但是这个时候确实不可使用，需要其他k块来恢复

exact repair

使用更小网络代价来修复是有可能的

> low rate
>
> high rate

这部分涉及到的工作还挺多的，有时间可以看一看具体内容

## 7. CONCLUSIONS

LRC降低带宽开销2倍，增加存储开销14%

提出了想法，应用在宽条带上，RS在宽条带上不可行，因为带宽要求随着块大小增长



# StripeMerge: Efficient Wide-Stripe Generation for Large-Scale Erasure-Coded Storage

## 0.Abstract

极限存储-宽条带 带宽开销巨大

StripeMerge 宽条带生成机制

将窄条带变成宽条带，这个过程中是为了最小化宽条带生成带宽

本文工作证明了最优方案的存在，还有两个启发式算法，生成时间减小87.8%

## 1.INTRODUCTION

为了保证data durability提出复制，但是冗余开销太大

RS是一种替代方案，冗余开销很低，但是现在还在追求更低的存储开销

宽条带

> 极端的存储冗余
>
>  more on low-cost storage durability than high data access performance
>
> 重构的时候，带宽消耗很严重

应该根据年龄不同，做不同的参数化，分层处理

> 数据块在新写入时往往被访问得更频繁，但随着它们的年龄变得更少
>
> 随着数据的老化，窄条带被重新编码为宽条带

将窄条带重新编码为宽条带不可避免地会重新定位数据块并重新生成校验块，从而导致数据传输中巨大的带宽开销

主要观点

宽条带的生成发生在有大量节点和条带的大型存储系统

选择两个窄条带合并成一个

 StripeMerge

## 2.BACKGROUND ANDMOTIVATION

### A Erasure Coding

讲了纠删码的概念，MDS码

### B Wide-Stripe Generation

宽条带的概念

假设

> 宽条带用于很少被访问的冷数据，比如备份和归档数据[1]、[6]或二进制大对象(blob)，它们的访问频率随着时间的推移而下降

宽条带有较高的修复代价，随着k增大而增大

宽条带生成问题

>宽条带生成步骤（两个窄条带生成一个宽条带）
>
>>重新分配2k块，分布在不同的节点
>>
>>将两个窄条带的一些数据和校验块迁移到一些负责生成宽条带的新校验块的节点上，它们的奇偶校验块稍后分布在不同的节点上。
>
>数据块的重新分布，从窄条带重新生成校验块，存在明显的数据传输
>
>目标最小化带宽

### C Storage Scaling

增加新节点扩容，为了在现有和新增加的节点上重新分配擦除编码块，他们研究了如何将(k,m)条转换为(k+s,m)条，以最小化伸缩带宽(即伸缩期间传输的数据量)

NCScale的扩展方法

![image-20211014093039882](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014093039882.png)

此时宽条带生成带宽依旧不为0，宽条带与存储扩展有所不同的是它不需要增加新的节点，这种差异导致它有不同的解决方案。

### D Our Idea

Perfect merging

> 从当前存储的大量窄条带中，选择两条合适的窄条带，用来合并为宽条带，不需要生成带宽

例子如下

![image-20211014095141389](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014095141389.png)

b和a的区别在于c ，d在单独的节点，并不需要再迁移，这个地方不消耗带宽

图示的两个条带有这样的特点，用来消除宽条带生成带宽

> 数据块在不同的节点
>
> > 因为宽条带最后数据块分布在不同的节点，如果有在相同节点的，那么会导致数据块迁移到其他节点，有带宽开销
>
> 校验块有相同的编码系数，并驻留在相同的节点，这些可以用来生成新的校验块

例子如下

![image-20211014095936799](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014095936799.png)

![image-20211014095943051](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014095943051.png)

由上面两个式子可以看出来，新的校验块可以通过之前的校验块本地生成，不产生带宽消耗

![image-20211014100208111](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014100208111.png)

**Challenges**：Perfect merging不产生带宽开销，但是如何在系统中应用Perfect merging生成多个宽条带。并且Perfect merging跟窄条带的放置方式有关。大规模存储系统中，搜索所有的窄条带会消耗大量时间。

## 3.ANALYSIS

SectionIII-A 我们将宽条带生成问题转换为bipartite graph model

SectionIII-B 证明在给定足够多的窄条带的情况下，总是存在一个可以生成所有宽条带而不需要任何宽条带生成带宽的最优方案。

### A Bipartite Graph Model

n个节点，足够多的(k,m)narrow stripes，目标是选择所有符合Perfect merging的窄条带，合并成(2k,m)的宽条带，这样就不存在跨节点的生成带宽

（k,m）条带，随机分布在k+m节点上

![image-20211014102642933](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014102642933.png)

总共排列方案，假设是每个都不同。但是实际上我们合并的时候，数据块顺序无所谓（我们计算的时候只用了校验块），校验块的顺序是重要的，对计算产生影响（主要是前面的系数）

![image-20211014104058750](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014104058750.png)

这种被认为是同一种摆放方式。

![image-20211014104323104](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014104323104.png)

所以总共可能性如上，被称为complete chunk placement set。

![image-20211014104423710](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014104423710.png)

提出了 bipartite graph

为什么x和y之间的节点连接起来就是完美匹配？

### B Existence

证明perfect merging的存在

证明没看懂

## 4.STRIPEMERGE

### A Limitations of Optimal Scheme

带宽问题，转化为二部图的最大匹配问题

最大匹配算法时间复杂度高，形成一个二部图在时间和空间上都是昂贵的，并且实际中，块数不一定一样，不一定能形成完美匹配。

### B Greedy Heuristic: StripeMerge-G

利用现有窄条纹的完美合并来生成尽可能多的宽条纹，而对于剩余的窄条纹，我们转移一些块使它们满足完美合并来生成剩余的宽条纹

merging cost 

> 转移块的数量

例子

![image-20211017124421315](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211017124421315.png)

设计算法：

![image-20211017124940726](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211017124940726.png)

思路

> stripemerg首先计算任意一对窄条纹的合并代价，并构造一个包含所有窄条纹对及其合并代价的集合(第1-7行)
>
> 根据代价排序，代价为0到k+m之间的整数，计数排序时间复杂度O(n<sup>2</sup>),n为窄条带数量，那么一共n(n-1)/2对，选择最小的合并，然后删除相关元素
>
> 时间复杂度为O((k+m)n<sup>2</sup>)并没有显著降低（主要是遍历求代价这部分）

### C Parity-aligned Heuristic: StripeMerge-P

完美合并的条带有以下特点

parity-aligned

> parity chunks have identical encoding coefficients and reside in identical nodes

目标是识别完全奇偶对齐的窄条纹对，从而快速获得满足完美合并的窄条纹对，显著减少了作为算法1输入的条纹数量，还识别partially parity-alignedpairs的条带块

构造一个集合校验块对齐的数量的集合，为了构造这个，将校验块的位置的元数据存储在哈希表中。

哈希表存储key-value，key指向奇偶块的具体拜访，value是具有相应奇偶块位置的条带的索引列表。O(1)时间查找到具有相同校验块位置的条带

对于任意条带，其校验块放置在m节点中，它可以为其所有奇偶校验块放置生成2<sup>m</sup> - 1个不同的键,这些会造成二外的内存开销，但是这个开销是有限的。

在StripeMerge-G基础上做了修改

具体算法

![image-20211017141058543](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211017141058543.png)



这种方法思想是找校验块对齐的条带，进行合并。但是他们的数据块可能在同一个节点，这样会造成数据迁移，cost增加

## 5.PERFORMANCE EVALUATION

 Amazon EC2

StripeMerge VS NCScale

比较点，节省多少带宽

 可以减少多少运行时间

### A Simulations

not implement coding operations and data transfers（没有数据传输这个带宽咋来的？？？）

> Intel Xeon Silver 4110 2.10 GHz CPU
>
>  256 GiB RAM
>
>  ST1000DM003 7200 RPM 1 TiB SATA hard disk
>
> 设N为2k+m的倍数

#### Experiment A.1 (Wide-stripe generation bandwidth)

10000个条带随机分布在N个节点，测试不同的N和（k，m）的组合

> StripeMerge-P ,StripeMerge-G, NCScale
>
> 4≤k≤64,2≤m≤4 N=2(2k+m) and N=4(2k+m)

![image-20211015162954707](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015162954707.png)

从图中可以看出来，带宽随着k和m的增加而增加，因为会使得完美匹配更加难满足

当k比较小时生成带宽随着N的增大而增大，k比较大时，随着N的增大而减小。

正面影响，N越大数据块越容易分布在不同的节点上，负面影响校验块越不容易驻留在相同节点。

k不大时，m和k差不都，负面影响大。k较大时，k远大于m，正面影响大，带宽减小。所以更大的k和N会受益（觉得缺乏说服力）

StripeMerge-P 和 StripeMerge-G性能差不多

#### Experiment A.2 (Running time versus(k,m))

StripeMerge-G and StripeMerge-P的运行时间对比（为啥不跟NCScale比，这个时间是0么）

![image-20211015164744703](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015164744703.png)

StripeMerge-P比StripeMerge-G 快，说明parity-aligned有效的加快了速度，k较小时，基本上是0.

优化幅度随着k的增大而减小，k越大，更多的数据块有糟糕的数据块布局，parity-aligned不能加速算法。

#### Experiment A.3 (Running time versus the number of narrow stripes)

测试算法运行时间与窄条纹数量

固定(k,m) = (16,4)andN=2(2k+m) =72

StripeMerge-G 随着条带数量显著增加（O((k+m)n<sup>2</sup> )），StripeMerge-P线性增加(O((k+m)mn))更适合大型系统

![image-20211015165442925](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015165442925.png)

#### Experiment A.4 (Memory consumption of StripeMerge-P)

total memory usage 

the memory usage of the hash table that stores parity-aligned metadata

fix(k,m) = (16,4) and N=2(2k+m) =72

![image-20211015181221294](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015181221294.png)

(注意轴是指数型的)，由此可见哈希表的开销和总内存开销相比是有限的，处理10000条窄条带需要4.85GB的内存，哈希表本身只需要72.5MB的内存，StripeMerge-P产生的额外开销是可以接受的。

### B Amazon EC2 Experiments

扩展了coding and data transfer功能，使用ISA-L实现编解码

N=2(2k+m) 个 m5.xlarge 实例 当存储节点，为了评估网络带宽的影响，专门配置了一个用作网关的专用实例。![image-20211015184350258](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015184350258.png)

来自实例的任何传输块都必须在到达另一个实例之前遍历网关。我们使用Linux流量控制命令来控制网关的输出带宽

实验中，将带宽从1Gb/s到8Gb/s变化，实验考虑不同的块大小 10,000 narrow

#### Experiment B.1 (Time breakdown)

时间分为3部分

1. 算法运行时间，找到需要合并的窄条带
2. 转移时间，指的是用于合并的块的转移
3. 计算时间，是指将窄条带的校验块合并为宽条带的新校验块的本地计算

> (k,m) = (16,4),N=2(2k+m) =72 
>
> chunk size of 64MiB 
>
> gateway bandwidth of 8Gb/s

![image-20211015190447156](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015190447156.png)

由表格可知传输时间占主导地位

stripemerg的运行时间占用了10,000条条纹总时间的1.65%，但这个百分比将随着条纹数量的增加而急剧增加(实验A.3)。因此，对于具有大量分条的大型存储系统，stripemerg的运行时间会降低整体性能。相比之下，stripemerg  - p的运行时间仅占10,000条带总时间的0.068%，而这一百分比只随着条带数量的增加而线性增加

#### Experiment B.2 (Wide-stripe generation time versus (k,m))

![image-20211015191921094](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015191921094.png)

gateway bandwidth as 8 Gb/s and the chunk size as 64 MiB

 k m 增加生成时间增加 ，完美匹配的少了，需要传输的数据多了，时间会变长

#### Experiment B.3 (Impact of gateway bandwidth)

(k,m) = (16,4),N=2(2k+m) =72

 gateway bandwidth, from 1 Gb/s to 8 Gb/s

![image-20211015192947798](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015192947798.png)

生成时间随着带宽增大而线性减小，并且StripeMerge 优于NCScale

#### Experiment B.4 (Impact of chunk size)

 chunk sizes, from 8 MiB to 64 MiB

(k,m) = (16,4),N=2(2k+m) =72 

gateway bandwidth of 8 Gb/s

 ![image-20211015193338865](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015193338865.png)

生成时间随着块大小线性增加，而且StripeMerge性能仍然较好

## 6.RELATEDWORK

在repair和scaling roblem最小化带宽的研究

repair

>locally repairable codes (LRC)
>
>> 通过额外的存储减小修复I/O
>
>regenerating codes(再生码)
>
>> 通过额外的存储减小修复I/O
>
>repair-efficient techniques
>
>> lazy recovery通过谨慎地延迟立即修复操作来减少修复流量
>
>parallelizing and pipelining repair
>
>> 减少修复时间
>
> scheduling repair tasks in free timeslots
>
>> 适应工作负载的动态变化

scaling problem

> minimize the bandwidth
>
> >  under RAID-0, RAID-5 (i.e., single fault tolerance) , or RS codes
>
> code conversion
>
> > 研究可转换的代码结构，以最小化代码转换中的I/O

本文研究的问题其实和上面的都不一样，研究如何最小化带宽并且减少宽条带生成问题的计算开销。

宽条带问题的相关研究

> VAST
>
> > locally decodable codes提高宽条带的修复性能
>
> Haddock et 
>
> > general-purpose GPUs 提高宽条带的解码效率
>
> ECWide
>
> > combined locality
> >
> > 系统的解决了修复带宽问题，提出了有效的编码和更新方案

本文关注点在于如何生成宽条带

## 7.CONCLUSIONS

StripeMerge生成宽条带

本文转换为bipartite graph modeling，证明了最优方案的存在。提出了两个算法，在有限带宽下生成（完美情况时间复杂度比较大），生成性能比现有性能好。

# Erasure Coding in Windows Azure Storage

## 0. Abstract

Windows Azure Storage (WAS)使用纠删码，引入了LRC的概念，减少修复所需要的块数，从而减小带宽和I/O 

## 1. Introduction

 (WAS)的一个简介

 stream layer

> append-only distributed file system

active-extents

>  replicated three times by the underlying stream layer. 

先写入三副本，达到一定大小，区域会被sealed，不能再被修改，成为纠删码的候选区，开始编码，编码结束，删除3副本

使用纠删码可以降低成本50%以上，存储马上达到EB，节省更多硬件，节省数据中心占地面积，节省电力。

trade-off是性能

dealing with

> a lost or offline data fragment
>
> hot storage nodes

数据不可用两种情况

> 丢失
>
> 所需数据在升级的节点上

重构返回数据给客户端，需要优化，优化方向

> networking bandwidth
>
> I/Os 
>
> 重构时间

纠删码，数据块存储在特定的节点上，增加了存储节点变热的风险，影响时延（疑问？有一篇说分片可以有效地减少负载不均衡）

WAS处理方法，识别热片段，存储到较冷的节点从而负载均衡，或者cache缓存数据，提供服务。

但是这样在完成迁移和cache缓存前，会影响性能，优化，如果读太久，直接重构。

所以对重构时间有要求，时间取决于最慢的节点

（12，4），重构成本大

减小读取片段数的好处

> reduces the network overhead and number of I/Os
>
> reduces the time it takes

引入了LRC实现（关键点在于98%的故障时单点故障，所以LRC很有用）

## 2. Local Reconstruction Codes

### 2.1 Definition

举了个（6.3）的例子

![image-20211020111728150](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020111728150.png)

LRC(k,l,r)

### 2.2 Fault Tolerance

容错的例子

构造特定系数实现Maximally Recoverable(MR) property

![image-20211020112525634](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020112525634.png)

#### 2.2.1 Constructing Coding Equations

![image-20211020112611275](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020112611275.png)

![image-20211020163149256](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020163149256.png)

这种情况下可以容任意3错，问题是如何求系数

关注以下情况

> None of the four parities fails
>
> Only one of p<sub>x</sub> and p<sub>y</sub> fails
>
> both p<sub>x</sub> and p<sub>y</sub> fails

这三种情况是可以解码的（具体内容可以之后再看看）

#### 2.2.2 Putting Things Together

可以对3中4错情况进行解码，占所有4故障的86%，实现了最大可恢复属性

#### 2.2.3 Checking Decodability

如何判断是否可恢复

对于每个本地组，如果校验块是可用的可以当作一个缺失的数据块，校验块标记为删除，完成本地组后，检查数据片段和全局校验块。如果缺失的数据片段数不超过全局校验块的数量，则理论上是可解的。否则不可解码。

### 2.3 Optimizing Storage Cost, Reliability and Performance

LRC(k, l, r)

> 单故障错误，修复需要k/l片段
>
> 可以容r+1错

n-k >= l+r

### 2.4 Summary

(k,r,l)容错在r+1 - r+l（能容多少错，取决于具体的故障情况）

## 3. Reliability Model and Code Selection

LRC的参数选择问题，可靠性要达到三副本的可靠性

### 3.1 Reliability Model

利用Markov models来进行判断

#### 3.1.1 Markov models

![image-20211020172107114](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020172107114.png)



## 6. Related Work

Erasure Coding in Storage Systems

> 很多系统使用了纠删码

Performance

> 在擦码存储系统中，节点故障会触发重构进程，导致重构读取时的延迟性能下降。此外，经验表明，没有数据丢失的瞬时错误占数据中心故障的90%以上
>
> 重构会导致读性能下降

Erasure Code Design

> 这部分没看懂，需要一会再看看

## 7. Summary

引入LRC 从（12，4）改为（12，2，2），比传统的三副本持久性好（应该没有（12，4）好）





