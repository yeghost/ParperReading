# XORing Elephants: Novel Erasure Codes for Big Data

## 0. ABSTRACT

RS基础上做了改进，通过增加存储冗余，优化性能

## 1. INTRODUCTION

介绍了多副本，纠删码。

纠删码的修复主要问题是，带宽开销，假如是（10，4），修复一块需要10倍块大小的带宽

提出了LRC的概念

## 1.1. Importance of Repair

分析了Facebook的相关数据得出降低网络带宽代价是重要的结论

efficiently repairable重要的原因

1. degraded reads

   > 很多瞬时错误并不会丢失永久性数据，但是会造成数据不可用，这个时候读编码的条块会读降级。
   >
   > 这个时候可以通过修复过程重建数据块，但是目的不是容错，而是为了更高的数据可用性。重建的块不必写入磁盘
   >
   > 所以efficient and fast repair 可以提高数据可用性

2. efficient node decommissioning

   >Hadoop可以让故障节点退役
   >
   >Functional data必须在退役之前从节点中复制出来，这是一个复杂且耗时的过程
   >
   >Fast repairs可以把节点退役视为定期修复，并且重新创建块，不会产生非常大的网络流量

3. repair influences the performance of other concurrent MapReduce jobs

   > 因为修复需要占用一定的网络带宽，与数据中心的网络带宽相比，存储空间的增长速度不成比例地快。所以这个问题会越来越严重，所以local repairs显得越发重要

4. local repair would be a key in facilitating geographically distributed file systems across data centers

   >RS在跨地理的程度上是不可行的 因为high bandwidth requirements across wide area networks
   >
   >local repair是可行的

复制可以很好的处理上面的问题，但是存储开销较大。MDS开销小，但是会遇到上面的问题。

本文可以看作是牺牲了一些存储效率，来达到其他指标

## 2. THEORETICAL CONTRIBUTIONS

MDS在通信和存储领域应用非常广泛

MDS是最低恢复冗余

两个定义

1. Minimum Code Distance
2. Block Locality

locality和good distance是矛盾的

LRC(k, n−k, r)

### 2.1. LRC implemented in Xorbas

Ci的选择有要求 线性无关，不能为0

设计了一个随机算法和确定算法，可以生成系数

有个优化S3不用存储，构造出来S1+S2+S3=0

系数的构造

## 3. SYSTEM DESCRIPTION

在HDFS-RAID上实现

> RaidNode 
>
> > 负责创建和维护校验块
>
> BlockFixer
>
> > 用来修复块
>
> ErasureCode
>
> > 实现编码和解码功能，上面两个组件都依赖于他

HDFS-Xorbas 在HDFS-RAID基础上增加了LRC

### 3.1. HDFS-Xorbas

#### 3.1.1. 编码

RaidNode 将一个文件分成10块，然后编码出4块。可能一个文件可能不够10块，默认剩下的填充为0，依旧是10块

![image-20211009150941798](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211009150941798.png)

LRC会额外计算两个块，如上图所示

#### 3.1.1.2 解码和修复

RaidNode

> light-decoder
>
> > 针对于每个条带单个块的错误
>
> heavy-decoder
>
> > light-decoder失败的时候使用

BlockFixer

> 检测到块失败，决定LRC恢复需要的5个块
>
>  light-decoding尝试恢复
>
> 如果出现multiple failures，可能没有需要的5个块， light-decoder失败， heavy-decoder启动
>
> heavy-decoder跟RS恢复过程一样，将结果发送和存储到数据节点

##  4.RELIABILITY ANALYSIS

 mean-time to data loss (MTTDL) 可靠性分析的依据

> 可以容忍的故障数
>
> 修复的速度
>
> 弹性增加和修复时间减小，MTTDL也会增加

结果LRC，RS比复制高了很多，但是复制的数据可用性比LRC和RS强

## 5. EVALUATION

两种环境性能

Amazon’s Elastic Compute Cloud

 a test cluster in Facebook

### 5.1 评价指标

HDFS Bytes Read

> 对应于为修复而启动的作业所读取的总数据量
>
> collected from the statistics-reports of the jobs spawned following a failure event

Network Traffic

> the total amount of data communicated from nodes in the cluster
>
> 单位为GB
>
> 用下面这个工具来监测
>
> Amazon’s A WS Cloudwatch monitoring tools

Repair Duration

>  the time interval between the starting time of the first repair job and the ending time of the last repair job.

### 5.2 EC2

two Hadoop clusters

> HDFS-Xorbas
>
> HDFS-RS

Each cluster

>  51 instances of type m1.small
>
> 1 master  hosting Hadoop’s NameNode, JobTracker and RaidNode daemons
>
> 50  slave as DataNode and a TaskTracker daemon

file size 640 MB

block size 64MB

每个文件两个集群中分别生成14和16块

故障包括一个数据节点或者多个数据节点的终止

四个故障事件是单节点错误，两个三节点错误，两个两节点错误

文件数量（20，100，200）

#### 5.2.1 HDFS Bytes Read

HDFS-Xorbas 比HDFS-RS好41%-52%

读的平均块数从11.5降到5.8

#### 5.2.2 Network Traffic

网络流量和读取的字节数基本上一致，二倍的关系

#### 5.2.3 Repair Time

Xorbas比HDFS-RS快25%到45%

实验里面带宽没满，实际环境中带宽可能跑满，时间表现可能更好

#### 5.2.4 Repair under Workload

为了演示修复性能对集群负载的影响。

创建了两个集群

每个集群15个从节点

块故障时不可用，LRC相比RS延迟小

### 5.3 Facebook’s cluster

区别点在于利用的集群中现有的数据集

块大小为256MB

94%文件3块 剩下10块 平均3.4块

由于块大小比较小，Xorbas比HDFS-RS存储开销大了27%（最好应为13%）

## 6. RELATED WORK

functional repair

> 虽然块可以恢复，但是这个时候确实不可使用，需要其他k块来恢复

exact repair

使用更小网络代价来修复是有可能的

> low rate
>
> high rate

这部分涉及到的工作还挺多的，有时间可以看一看具体内容

## 7. CONCLUSIONS

LRC降低带宽开销2倍，增加存储开销14%

提出了想法，应用在宽条带上，RS在宽条带上不可行，因为带宽要求随着块大小增长



# StripeMerge: Efficient Wide-Stripe Generation for Large-Scale Erasure-Coded Storage

## 0.Abstract

极限存储-宽条带 带宽开销巨大

StripeMerge 宽条带生成机制

将窄条带变成宽条带，这个过程中是为了最小化宽条带生成带宽

本文工作证明了最优方案的存在，还有两个启发式算法，生成时间减小87.8%

## 1.INTRODUCTION

为了保证data durability提出复制，但是冗余开销太大

RS是一种替代方案，冗余开销很低，但是现在还在追求更低的存储开销

宽条带

> 极端的存储冗余
>
>  more on low-cost storage durability than high data access performance
>
> 重构的时候，带宽消耗很严重

应该根据年龄不同，做不同的参数化，分层处理

> 数据块在新写入时往往被访问得更频繁，但随着它们的年龄变得更少
>
> 随着数据的老化，窄条带被重新编码为宽条带

将窄条带重新编码为宽条带不可避免地会重新定位数据块并重新生成校验块，从而导致数据传输中巨大的带宽开销

主要观点

宽条带的生成发生在有大量节点和条带的大型存储系统

选择两个窄条带合并成一个

 StripeMerge

## 2.BACKGROUND ANDMOTIVATION

### A Erasure Coding

讲了纠删码的概念，MDS码

### B Wide-Stripe Generation

宽条带的概念

假设

> 宽条带用于很少被访问的冷数据，比如备份和归档数据[1]、[6]或二进制大对象(blob)，它们的访问频率随着时间的推移而下降

宽条带有较高的修复代价，随着k增大而增大

宽条带生成问题

>宽条带生成步骤（两个窄条带生成一个宽条带）
>
>>重新分配2k块，分布在不同的节点
>>
>>将两个窄条带的一些数据和校验块迁移到一些负责生成宽条带的新校验块的节点上，它们的奇偶校验块稍后分布在不同的节点上。
>
>数据块的重新分布，从窄条带重新生成校验块，存在明显的数据传输
>
>目标最小化带宽

### C Storage Scaling

增加新节点扩容，为了在现有和新增加的节点上重新分配擦除编码块，他们研究了如何将(k,m)条转换为(k+s,m)条，以最小化伸缩带宽(即伸缩期间传输的数据量)

NCScale的扩展方法

![image-20211014093039882](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014093039882.png)

此时宽条带生成带宽依旧不为0，宽条带与存储扩展有所不同的是它不需要增加新的节点，这种差异导致它有不同的解决方案。

### D Our Idea

Perfect merging

> 从当前存储的大量窄条带中，选择两条合适的窄条带，用来合并为宽条带，不需要生成带宽

例子如下

![image-20211014095141389](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014095141389.png)

b和a的区别在于c ，d在单独的节点，并不需要再迁移，这个地方不消耗带宽

图示的两个条带有这样的特点，用来消除宽条带生成带宽

> 数据块在不同的节点
>
> > 因为宽条带最后数据块分布在不同的节点，如果有在相同节点的，那么会导致数据块迁移到其他节点，有带宽开销
>
> 校验块有相同的编码系数，并驻留在相同的节点，这些可以用来生成新的校验块

例子如下

![image-20211014095936799](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014095936799.png)

![image-20211014095943051](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014095943051.png)

由上面两个式子可以看出来，新的校验块可以通过之前的校验块本地生成，不产生带宽消耗

![image-20211014100208111](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014100208111.png)

**Challenges**：Perfect merging不产生带宽开销，但是如何在系统中应用Perfect merging生成多个宽条带。并且Perfect merging跟窄条带的放置方式有关。大规模存储系统中，搜索所有的窄条带会消耗大量时间。

## 3.ANALYSIS

SectionIII-A 我们将宽条带生成问题转换为bipartite graph model

SectionIII-B 证明在给定足够多的窄条带的情况下，总是存在一个可以生成所有宽条带而不需要任何宽条带生成带宽的最优方案。

### A Bipartite Graph Model

n个节点，足够多的(k,m)narrow stripes，目标是选择所有符合Perfect merging的窄条带，合并成(2k,m)的宽条带，这样就不存在跨节点的生成带宽

（k,m）条带，随机分布在k+m节点上

![image-20211014102642933](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014102642933.png)

总共排列方案，假设是每个都不同。但是实际上我们合并的时候，数据块顺序无所谓（我们计算的时候只用了校验块），校验块的顺序是重要的，对计算产生影响（主要是前面的系数）

![image-20211014104058750](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014104058750.png)

这种被认为是同一种摆放方式。

![image-20211014104323104](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014104323104.png)

所以总共可能性如上，被称为complete chunk placement set。

![image-20211014104423710](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014104423710.png)

提出了 bipartite graph

为什么x和y之间的节点连接起来就是完美匹配？

### B Existence

证明perfect merging的存在

证明没看懂

## 4.STRIPEMERGE

### A Limitations of Optimal Scheme

带宽问题，转化为二部图的最大匹配问题

最大匹配算法时间复杂度高，形成一个二部图在时间和空间上都是昂贵的，并且实际中，块数不一定一样，不一定能形成完美匹配。

### B Greedy Heuristic: StripeMerge-G

利用现有窄条纹的完美合并来生成尽可能多的宽条纹，而对于剩余的窄条纹，我们转移一些块使它们满足完美合并来生成剩余的宽条纹

merging cost 

> 转移块的数量

例子

![image-20211017124421315](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211017124421315.png)

设计算法：

![image-20211017124940726](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211017124940726.png)

思路

> stripemerg首先计算任意一对窄条纹的合并代价，并构造一个包含所有窄条纹对及其合并代价的集合(第1-7行)
>
> 根据代价排序，代价为0到k+m之间的整数，计数排序时间复杂度O(n<sup>2</sup>),n为窄条带数量，那么一共n(n-1)/2对，选择最小的合并，然后删除相关元素
>
> 时间复杂度为O((k+m)n<sup>2</sup>)并没有显著降低（主要是遍历求代价这部分）

### C Parity-aligned Heuristic: StripeMerge-P

完美合并的条带有以下特点

parity-aligned

> parity chunks have identical encoding coefficients and reside in identical nodes

目标是识别完全奇偶对齐的窄条纹对，从而快速获得满足完美合并的窄条纹对，显著减少了作为算法1输入的条纹数量，还识别partially parity-alignedpairs的条带块

构造一个集合校验块对齐的数量的集合，为了构造这个，将校验块的位置的元数据存储在哈希表中。

哈希表存储key-value，key指向奇偶块的具体拜访，value是具有相应奇偶块位置的条带的索引列表。O(1)时间查找到具有相同校验块位置的条带

对于任意条带，其校验块放置在m节点中，它可以为其所有奇偶校验块放置生成2<sup>m</sup> - 1个不同的键,这些会造成二外的内存开销，但是这个开销是有限的。

在StripeMerge-G基础上做了修改

具体算法

![image-20211017141058543](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211017141058543.png)



这种方法思想是找校验块对齐的条带，进行合并。但是他们的数据块可能在同一个节点，这样会造成数据迁移，cost增加

## 5.PERFORMANCE EVALUATION

 Amazon EC2

StripeMerge VS NCScale

比较点，节省多少带宽

 可以减少多少运行时间

### A Simulations

not implement coding operations and data transfers（没有数据传输这个带宽咋来的？？？）

> Intel Xeon Silver 4110 2.10 GHz CPU
>
>  256 GiB RAM
>
>  ST1000DM003 7200 RPM 1 TiB SATA hard disk
>
> 设N为2k+m的倍数

#### Experiment A.1 (Wide-stripe generation bandwidth)

10000个条带随机分布在N个节点，测试不同的N和（k，m）的组合

> StripeMerge-P ,StripeMerge-G, NCScale
>
> 4≤k≤64,2≤m≤4 N=2(2k+m) and N=4(2k+m)

![image-20211015162954707](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015162954707.png)

从图中可以看出来，带宽随着k和m的增加而增加，因为会使得完美匹配更加难满足

当k比较小时生成带宽随着N的增大而增大，k比较大时，随着N的增大而减小。

正面影响，N越大数据块越容易分布在不同的节点上，负面影响校验块越不容易驻留在相同节点。

k不大时，m和k差不都，负面影响大。k较大时，k远大于m，正面影响大，带宽减小。所以更大的k和N会受益（觉得缺乏说服力）

StripeMerge-P 和 StripeMerge-G性能差不多

#### Experiment A.2 (Running time versus(k,m))

StripeMerge-G and StripeMerge-P的运行时间对比（为啥不跟NCScale比，这个时间是0么）

![image-20211015164744703](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015164744703.png)

StripeMerge-P比StripeMerge-G 快，说明parity-aligned有效的加快了速度，k较小时，基本上是0.

优化幅度随着k的增大而减小，k越大，更多的数据块有糟糕的数据块布局，parity-aligned不能加速算法。

#### Experiment A.3 (Running time versus the number of narrow stripes)

测试算法运行时间与窄条纹数量

固定(k,m) = (16,4)andN=2(2k+m) =72

StripeMerge-G 随着条带数量显著增加（O((k+m)n<sup>2</sup> )），StripeMerge-P线性增加(O((k+m)mn))更适合大型系统

![image-20211015165442925](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015165442925.png)

#### Experiment A.4 (Memory consumption of StripeMerge-P)

total memory usage 

the memory usage of the hash table that stores parity-aligned metadata

fix(k,m) = (16,4) and N=2(2k+m) =72

![image-20211015181221294](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015181221294.png)

(注意轴是指数型的)，由此可见哈希表的开销和总内存开销相比是有限的，处理10000条窄条带需要4.85GB的内存，哈希表本身只需要72.5MB的内存，StripeMerge-P产生的额外开销是可以接受的。

### B Amazon EC2 Experiments

扩展了coding and data transfer功能，使用ISA-L实现编解码

N=2(2k+m) 个 m5.xlarge 实例 当存储节点，为了评估网络带宽的影响，专门配置了一个用作网关的专用实例。![image-20211015184350258](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015184350258.png)

来自实例的任何传输块都必须在到达另一个实例之前遍历网关。我们使用Linux流量控制命令来控制网关的输出带宽

实验中，将带宽从1Gb/s到8Gb/s变化，实验考虑不同的块大小 10,000 narrow

#### Experiment B.1 (Time breakdown)

时间分为3部分

1. 算法运行时间，找到需要合并的窄条带
2. 转移时间，指的是用于合并的块的转移
3. 计算时间，是指将窄条带的校验块合并为宽条带的新校验块的本地计算

> (k,m) = (16,4),N=2(2k+m) =72 
>
> chunk size of 64MiB 
>
> gateway bandwidth of 8Gb/s

![image-20211015190447156](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015190447156.png)

由表格可知传输时间占主导地位

stripemerg的运行时间占用了10,000条条纹总时间的1.65%，但这个百分比将随着条纹数量的增加而急剧增加(实验A.3)。因此，对于具有大量分条的大型存储系统，stripemerg的运行时间会降低整体性能。相比之下，stripemerg  - p的运行时间仅占10,000条带总时间的0.068%，而这一百分比只随着条带数量的增加而线性增加

#### Experiment B.2 (Wide-stripe generation time versus (k,m))

![image-20211015191921094](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015191921094.png)

gateway bandwidth as 8 Gb/s and the chunk size as 64 MiB

 k m 增加生成时间增加 ，完美匹配的少了，需要传输的数据多了，时间会变长

#### Experiment B.3 (Impact of gateway bandwidth)

(k,m) = (16,4),N=2(2k+m) =72

 gateway bandwidth, from 1 Gb/s to 8 Gb/s

![image-20211015192947798](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015192947798.png)

生成时间随着带宽增大而线性减小，并且StripeMerge 优于NCScale

#### Experiment B.4 (Impact of chunk size)

 chunk sizes, from 8 MiB to 64 MiB

(k,m) = (16,4),N=2(2k+m) =72 

gateway bandwidth of 8 Gb/s

 ![image-20211015193338865](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015193338865.png)

生成时间随着块大小线性增加，而且StripeMerge性能仍然较好

## 6.RELATEDWORK

在repair和scaling roblem最小化带宽的研究

repair

>locally repairable codes (LRC)
>
>> 通过额外的存储减小修复I/O
>
>regenerating codes(再生码)
>
>> 通过额外的存储减小修复I/O
>
>repair-efficient techniques
>
>> lazy recovery通过谨慎地延迟立即修复操作来减少修复流量
>
>parallelizing and pipelining repair
>
>> 减少修复时间
>
> scheduling repair tasks in free timeslots
>
>> 适应工作负载的动态变化

scaling problem

> minimize the bandwidth
>
> >  under RAID-0, RAID-5 (i.e., single fault tolerance) , or RS codes
>
> code conversion
>
> > 研究可转换的代码结构，以最小化代码转换中的I/O

本文研究的问题其实和上面的都不一样，研究如何最小化带宽并且减少宽条带生成问题的计算开销。

宽条带问题的相关研究

> VAST
>
> > locally decodable codes提高宽条带的修复性能
>
> Haddock et 
>
> > general-purpose GPUs 提高宽条带的解码效率
>
> ECWide
>
> > combined locality
> >
> > 系统的解决了修复带宽问题，提出了有效的编码和更新方案

本文关注点在于如何生成宽条带

## 7.CONCLUSIONS

StripeMerge生成宽条带

本文转换为bipartite graph modeling，证明了最优方案的存在。提出了两个算法，在有限带宽下生成（完美情况时间复杂度比较大），生成性能比现有性能好。

# Erasure Coding in Windows Azure Storage

## 0. Abstract

Windows Azure Storage (WAS)使用纠删码，引入了LRC的概念，减少修复所需要的块数，从而减小带宽和I/O 

## 1. Introduction

 (WAS)的一个简介

 stream layer

> append-only distributed file system

active-extents

>  replicated three times by the underlying stream layer. 

先写入三副本，达到一定大小，区域会被sealed，不能再被修改，成为纠删码的候选区，开始编码，编码结束，删除3副本

使用纠删码可以降低成本50%以上，存储马上达到EB，节省更多硬件，节省数据中心占地面积，节省电力。

trade-off是性能

dealing with

> a lost or offline data fragment
>
> hot storage nodes

数据不可用两种情况

> 丢失
>
> 所需数据在升级的节点上

重构返回数据给客户端，需要优化，优化方向

> networking bandwidth
>
> I/Os 
>
> 重构时间

纠删码，数据块存储在特定的节点上，增加了存储节点变热的风险，影响时延（疑问？有一篇说分片可以有效地减少负载不均衡）

WAS处理方法，识别热片段，存储到较冷的节点从而负载均衡，或者cache缓存数据，提供服务。

但是这样在完成迁移和cache缓存前，会影响性能，优化，如果读太久，直接重构。

所以对重构时间有要求，时间取决于最慢的节点

（12，4），重构成本大

减小读取片段数的好处

> reduces the network overhead and number of I/Os
>
> reduces the time it takes

引入了LRC实现（关键点在于98%的故障时单点故障，所以LRC很有用）

## 2. Local Reconstruction Codes

### 2.1 Definition

举了个（6.3）的例子

![image-20211020111728150](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020111728150.png)

LRC(k,l,r)

### 2.2 Fault Tolerance

容错的例子

构造特定系数实现Maximally Recoverable(MR) property

![image-20211020112525634](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020112525634.png)

#### 2.2.1 Constructing Coding Equations

![image-20211020112611275](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020112611275.png)

![image-20211020163149256](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020163149256.png)

这种情况下可以容任意3错，问题是如何求系数

关注以下情况

> None of the four parities fails
>
> Only one of p<sub>x</sub> and p<sub>y</sub> fails
>
> both p<sub>x</sub> and p<sub>y</sub> fails

这三种情况是可以解码的（具体内容可以之后再看看）

#### 2.2.2 Putting Things Together

可以对3中4错情况进行解码，占所有4故障的86%，实现了最大可恢复属性

#### 2.2.3 Checking Decodability

如何判断是否可恢复

对于每个本地组，如果校验块是可用的可以当作一个缺失的数据块，校验块标记为删除，完成本地组后，检查数据片段和全局校验块。如果缺失的数据片段数不超过全局校验块的数量，则理论上是可解的。否则不可解码。

### 2.3 Optimizing Storage Cost, Reliability and Performance

LRC(k, l, r)

> 单故障错误，修复需要k/l片段
>
> 可以容r+1错

n-k >= l+r

### 2.4 Summary

(k,r,l)容错在r+1 - r+l（能容多少错，取决于具体的故障情况）

## 3. Reliability Model and Code Selection

LRC的参数选择问题，可靠性要达到三副本的可靠性

### 3.1 Reliability Model

利用Markov models来进行判断

#### 3.1.1 Markov models

![image-20211020172107114](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020172107114.png)![image-20211024160740251](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211024160740251.png)

利用马尔可夫链求出了MTTF

比三副本和（6，3）都要高，因为容任意三错，并且可以容86%的4错

### 3.2 Cost and Performance Trade-offs

![image-20211024161613591](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211024161613591.png)

每个点代表一组参数

下界，代表相同存储冗余。重建开销较小。

### 3.3 Code Parameter Selection



![image-20211024162130837](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211024162130837.png)

### 3.4 Comparison - Modern Storage Codes

Weaver codes

HoVer codes

Stepped Combination codes

这三种编码回头可以看看

![image-20211024163631562](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211024163631562.png)

性能好处在于

> 校验分为本地和全局的
>
> > 本地校验只涉及最小的数据片段
> >
> > 全局涉及所有数据片段，可以提供多个容错性

其他编码都是相同的责任，重构和容错

### 3.5 Correlated Failures

马尔可夫链 要求是独立的，所以我们分别放置在不同的地方。如果有例外的情况要在链上添加弧。

## 4. Erasure Coding Implementation in WAS

分为三层

> front-end layer
>
> partitioned object layer
> stream replication layer（EC实现）这个地方有个东西需要再看看

### 4.1 Stream Layer Architecture

Stream Managers

![image-20211023153158529](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211023153158529.png)

Streams

> save a list of **extents**
>
> >  a list of append blocks
> >
> > > each block CRC(循环冗余校验（Cyclic Redundancy Check， *CRC*）)
> >
> > extent is replicated on multiple (usually three) ENs
>
>  Write operations for a stream keep appending to an extent
>
> > the extent reaches its maximum size (in the range of 1GB-3GB)
> >
> > there is a failure in the replica set

### 4.2 Erasure Coding in the Stream Layer

erasure coding process

> completely asynchronous
>
> off the critical path of client writes

1.  SM creates fragments on a set of ENs
2. SM designates one of the ENs in the extent’s replica set as the coordinator of erasure coding
3. sends it the metadata for the replica set

![image-20211023194815165](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211023194815165.png)

### 4.3 Using Local Reconstruction Codes in Windows Azure Storage
 placement of the fragments

> load 为了负载均衡
>
> reliability 尽量不放在相同域

涉及到一个升级域的概念 需要再研究下

### 4.4 Designing for Erasure Coding

**Scheduling of Various I/O Types**

stream layer handles a large mix of I/O types

> open/close, read, and append operations from clients, create, delete, replicate, reconstruct, scrub, and move operations generated by the system itself

这么多操作，存在调度问题

**Reconstruction Read-ahead and Caching**

等不可用片段达到一定大小再开始重建，减少磁盘和I/O的数量

**Consistency of Coded Data**

检查数据一致性

Checksum and parity是防止数据损坏的两个主要机制

## 5. Performance

与RS码在大小两种I/O下进行比较

LRC（12，2，2）和RS(12,4)进行比较

（12，3）的可靠性低于三副本

### 5.1 Small I/Os

key metric(4KB to64KB range)

> latency
>
>  the number of I/Os taken

负载较轻时，没有太大区别。

负载较重时，读取很多块，延迟取决于最慢的片段，选择更多的数据片段。有效降低延迟。

LRC读的块数少延迟低，虽然比更多的片段延迟大，但是带宽节省更多

![image-20211022190238660](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211022190238660.png)

### 5.2 Large I/Os

4MB large I/Os(4MB large I/Os)

>  latency
>
> bandwidth consumption

负载较轻时，时延仍然增大

延迟在网络和磁盘带宽

 1Gbps network

LRC由于减少了数据传输量，性能提高，片段越多反而性能越差



![image-20211022193653754](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211022193653754.png)

### 5.3 Decoding Latency

LRC比RS解码快，但是解码比传输小几个数量级，并不重要。



## 6. Related Work

Erasure Coding in Storage Systems

> 很多系统使用了纠删码

Performance

> 在擦码存储系统中，节点故障会触发重构进程，导致重构读取时的延迟性能下降。此外，经验表明，没有数据丢失的瞬时错误占数据中心故障的90%以上
>
> 重构会导致读性能下降

Erasure Code Design

> 这部分没看懂，需要一会再看看

## 7. Summary

引入LRC 从（12，4）改为（12，2，2），比传统的三副本持久性好（应该没有（12，4）好）

# Network Coding for Distributed Storage Systems

## 0.abstract

RS码通过更少的冗余达到相同的可靠性水平，但是网络传输量大。本文提出了再生码的概念，可以显著的降低带宽。

storage and repair bandwidth有一个权衡，本文对他进行了描述

## 1.introduction

分布式存储系统通过分布式的存储节点来长期可靠地存储数据，单个节点存储是不可靠的。

一些相关的存储系统

> OceanStore
>
> Total Recall 
>
> DHash++ 

提供可靠性，都需要增加冗余，复制是一种方法，RS冗余更小。相同冗余下，可靠性比复制高几个数量级

节点故障或者离开系统，冗余不断刷新，涉及到网络中大量的数据传输

![image-20211028112546349](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211028112546349.png)

生成一个M/K的片段，传输量需要M，事实上所有的已知编码结构都需要访问原始数据对象来生成编码片段

本文中，展示了修复可以不与整个对象通信。

2MB对象，（4，2）可以通过1.5MB进行修复， information theoretic minimum

确定了tradeoff在存储和修复带宽上，提出了再生码的概念，纠正了之前文献中的一个计算错误

两个极值点MDS,MBR

MBR有最小的修复带宽如果每个节点存储的多余M/k，修复带宽会显著下降

讲解了下本文的组织结构

## 2. BACKGROUND ANDRELA TEDWORK

### A.Erasure Codes

 tradeoff

> redundancy
>
> error tolerance

在冗余和可靠性权衡下，RS是最优的。

近期的一些研究关注在其他指标上

> sparse graph codes 
>
> > 性能接近最有，编码和解码复杂度较低
>
> parity array codes
>
> > 基于异或操作，目标是低解码，编码，更新复杂度。

本文的侧重点，指标上增加了带宽的指标。

### B.Network Coding

网络编码是传统路由(存储转发)方法的推广

允许中间节点通过编码，来生成输出数据，允许数据在中间节点混合。

本文考虑了网络编码在分布式修复问题中的应用，考虑修复带宽和存储之间的权衡。

### C.Distributed Storage Systems

之前的一些工作，对比了复制和RS的性能

Hybrid strategy

> 一个特殊节点除了维护多个被擦除的片段，还维护一个完整的副本
>
> 这个节点可以产生新的片段，并且发送给新来者
>
> 但是会降低带宽效率，并且使系统设计变复杂

RS存储效益好，但是带宽成本太高

混合策略不现实

## 3.ANALYSIS

我们的分析基于信息流图G，该图描述了数据对象的信息如何通过网络进行通信，存储在内存有限的节点中，并到达数据收集器的重构点

### A. Information Flow Graph

有向无环图

> 源数据节点 S
>
> 存储节点 x<sup>i</sup><sub>in</sub>, x<sup>i</sup><sub>out</sub>
>
> 数据收集者DC<sub>i</sub>
>
> 系统中的存储节点由 x<sup>i</sup><sub>in</sub>, x<sup>i</sup><sub>out</sub>表示，这两个节点由一条有向边连接，容量等于存储在节点上的数据量。
>
> 节点要么是活动的，要么是不活动的，取决于是否可用

具体变化

> 初始时，只有源节点是活动的，然后它接触一组初始的存储节点，用有向边连接到 x<sup>i</sup><sub>in</sub>，此时源节点变得不活动
>
> 存储节点变得活跃，代表一个分布式EC。新节点加入系统只能与活动节点相连。
>
> minimum cuts
>
> > 

![image-20211031183002355](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211031183002355.png)



### B. Storage-Bandwidth Tradeoff

n节点存储a个字节，一个节点失败，一个新节点会跟d个节点每个通信B字节，总修复带宽是y=dB

（n,k,d,a,y）

中间有个很复杂的证明过程



### C. Special Cases: Minimum-Storage Regenerating (MSR) Codes and Minimum-Bandwidth Regenerating (MBR) Codes


## 5.CONCLUSION

本文提出了一个理论框架，可以确定在编码系统中修复必须要通信的信息，并确定存储和修复带宽之间的权衡。

未来计划

> investigate deterministic designs of regenerating codes over small finite fields, the existence of systematic regenerating codes
>
> designs that minimize the overhead storage of the coefficients, as well as the impact of node dynamics in reliability

再生码，一个潜在应用领域是分布式归档存储或备份，其中的文件通常较大且不常被读取。

在这种情况下，再生码会在冗余、可靠性和修复带宽方面提供理想的折衷



## 6.附录



# Geometric Partitioning: Explore the Boundary of Optimal Erasure Code Repair
## 0.Abstract

再生码是特殊的纠删码，目的是减少修复所需的数据量

> 再生码的修复粒度是块而不是字节（纠删码都是按照块修的吧？）
>
> 块大小的选择会导致流降级读时间和修复吞吐量之间的紧张关系（关键在块大小）

文章为了解决这个问题提出了Geometric Partitioning，将每个对象按照其大小和geometric顺序分成一系列块，来获得大块和小块的好处（那是不是同时存在大块和小块的坏处，所以要想办法规避这个问题？），可以达到1.85x RS的性能，同时保持较低的读取时间。

## 1.Introduction

提了一些经典的对象存储系统

给了个概念修复成本：由于数据丢失而需要修复的数据量

引出了再生码，修复成本最低

Recovery efficiency

> 系统以多快的速度恢复其原来的容错能力

**修复成本的最优化并不一定能提高恢复效率或减少降级的读取时间**

> 为了修复一个块，只从相应的磁盘中读取一小部分子块。这种分散的磁盘访问模式会导致碎片化，从而降低磁盘性能，从而降低恢复效率

分块修复粒度还会增加降级的读时间，RS可以并行修复，再生码需要等待第一个块修复，可能需要与修复对象一样多的时间

当读取小于chunk大小的对象时，也会修复不必要的数据，导致读放大，降低读时间

再生码，对象存储系统通常选择单个块大小作为编码单元，编码前将对象分割为块。

> 较大的块大小有助于减少磁盘访问时的碎片和不连续读取，从而提高恢复效率。
>
> 但是较大的块大小会增加等待第一个修复块的时间，增加读放大的机会

低退化读时间和高恢复效率可以同时实现

> 关键是在每个对象中使用可变的块大小
>
> 我们从小块开始修复，以避免不必要的等待第一个块的修复，然后限制相邻块的大小比例，使当前块的修复可以早于前一个块的转移

Geometric Partitioning

> 它将每个对象按照几何顺序(例如:例如4MB、8MB、16MB、32MB、64MB等)，并将来自不同对象的块分组到桶中进行编码
>
> 比最小块大小更小的对象，使用RS编码，来消除读放大

## 2.Background

### 2.1 Repair, Degraded Read and Recovery

修复

> 降级读
>
> > 时间是重要指标，修复和传输可以pipelined
> >
> > 降级读时间为传输时间加上第一个块的修复时间
> >
> > 有效的修复可以降低MTTL，增加系统的耐久性，减少降级读的次数减轻系统的负担。
>
> 出现故障
>
> > 恢复效率由吞吐量决定，磁盘带宽成为了恢复吞吐量的决定性因素，因为磁盘带宽更难以充分利用。



# Clay Codes: Moulding MDS Codes to Yield an MSR Code

## 0.Abstract

为了保证可靠性，引入RS码，MDS码满足存储需求，但是不满足其他需要（我理解主要是网络），MSR码很多都是只有理论结构，clay code是MSR码，通过在任意单个MDS代码的多个堆叠层之间使用成对耦合，提供了一种简化的解码/修复结构。（设计思想，这部分还需要再看看）

clay code提供了MSR码的第一个实际实现

> 低存储开销
>
> 在三个关键参数方面同时优化:修复带宽、子分组级别和磁盘I/O
>
> 数据和奇偶校验节点的统一修复性能
>
> 支持单节点和多节点修复，同时允许更快和更有效的修复

MSR是**向量码**（一个新的概念）

本文对clay code性能进行了评估，存储开销增加1.25x，带宽减少2.9倍

## 1.Introduction

介绍了MDS码的优点，节省存储容量。介绍了缺点，修复成本高，增加带宽和修复时间。

MSR具有MDS所有优点，并且需要较小的修复带宽。

但是对于实际系统来说缺乏几个关键属性

> 计算上更加复杂
>
> 对不同类型的节点故障表现出不一致的修复特征
>
> 有限的容错（1，2）
>
> 缺乏常用RS的结构

本文提出了扩展之前理论的Clay codes

主要思想

> Clay codes are constructed by placing any MDS code in multiple layers and performing pair-wise coupling across layers.

本文实现了Clay,集成到了Ceph

存储开销为1.25x，修复网络流量降低2.9倍

## 2.Background and Preliminaries

**Erasure Code** 

> 介绍了下有限域，利用上标区分矢量与标量

**Scalar Codes**

> 每个chunk由L个字节组成，标量码，每个块中选择1字节，总共k字节，通过m种不同的方式，得到m字节，n=k+m称之为码字，重复L次，创造L个码字

**Vector Codes**

> 使用的是α > 1的有序集合
>
> 称之为superbyte
>
> 编码过程中，从k个数据块中挑选一个超字节，并且以m不同的方式线性组合这些超字节，以获得m校验超字节
>
> n=k+m叫做 (vector) codeword
>
> N=L/α
>
> α是超字节的子分组级
>
> Scalar Codes可以认为是α为1
>
> Vector Codes可以认为用一个向量码替代α标量码字
>
> 故障节点中的编码修复可以通过访问超字节中α字节的一个子集来实现
>
> 故障节点中的编码块的修复可以通过访问超字节中α字节的一个子集来实现，该子集存在于每个剩余的编码块中
>
> 减少了由于节点修复而引起的网络流量

![image-20211112190803451](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211112190803451.png)

**Sub-chunking through Interleaving**

> 上图与超字节相关联的α字节被连续存储，当sub-packetization level α比较大时，考虑到涉及多个码字的操作时并行执行的，从易于记忆访问的观点，交叉字节是有利的，以便不同码字的对应字节被连续存储

![image-20211112193942508](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211112193942508.png)

> 当一个块中的超字节数很大时，通过交叉，每个数据块被划分为α子集，称之为子块，因此，节点内的每个子块从存储在节点内的N个码字中的每个字持有一个字节。（这部分不太理解）

**MDS Codes**

无论时矢量还是标量，（n,k）都可以从任何n-k节点恢复，称之为MDS，有最小的冗余开销。

>  RS
>
> Row-Diagonal Parity 
>
> EVENODD 

**Node Repair**

节点修复操作，产生了大量的网络流量，RS码，流量消耗大

**MSR Codes**

Vector MDS的一种，具有最小的可能修复带宽

介绍了一些相关的参数

除了MSR低存储开销和低带宽开销的特点，还希望具有以下特点

>uniform-repair capability
>
>minimal disk read
>
>low value of sub-packetization parameter α
>
>a small size of underlying finite field over which the code is constructed

### 2.1 Related Work

节点有效修复工作

> Locally repairable codes
>
> Xorbas
>
> piggy-backed RS codes

都没达到MSR的属性

目前各种MSR都没达到所有想要的性能

![image-20211112201409926](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211112201409926.png)

FMSR codes

> 允许有效的修复，但是重构数据的功能不一定与故障数据相同，需要执行额外的解码操作来检索原始数据

Butterfly

> 通过实验验证了减少数据下载对节点修复的理论证明好处
>
> sub-packetization比较大
>
> m=2，对修复带宽限定较大

HashTag

> α≤(n−k)<sup>k/n−k</sup>
>
> 允许以修复带宽为代价灵活的选择α
>
> 仅支持系统节点的有效修复，需要在辅助节点上进行计算，并且涉及大的有限域运算

product-matrixMSR

> 具有非常低的子分组
>
> 很小的有限域大小
>
> 但是需要很大的存储开销

zig-zag codes

> 提出了第一个理论结构
>
> 当d=n-1 时，每个n，k的低存储开销的MSR码
>
> 构造是**非显式**的，决定parities的有限域系数必须由计算机搜索找到

尽管有许多理论构造和较少的实际实现，但寻找具有上述所有理想性质和实际评估的MSR代码仍然是难以实现的

 Ye and Barg的理论，改变了之前的情况

> 提供了一种结构，允许存储开销尽可能接近于1，子分组水平接近于可能的最小值，有限域大小不大于n，最佳磁盘I/O和全节点最优修复

Clay code实现了上述理论的结构，并且还具有一些额外的优势。

### 2.2 Refinements over Ye-Barg Code

Clay code

> 从耦合层的角度实现
>
> 数据解码算法和节点修复算法，都可以用两种操作
>
> > scalar MDS code
> >
> > 字节对之间的基本线性变换（Ye-Barg Code隐式，Clay Code显式）

Clay使用任意MDS，Ye-Barg code基于Vandermonde-RS codes

在耦合层架构中使用相同的MDS代码，并获得MSR的额外好处，Clay Code提出了一种用于修复多个故障的通用算法，允许我们在减少修复带宽的情况下修复多个节点。我们的改进针对于实现。

## 3. Construction of the Clay Code

**Single Codeword Description** 

> N=1,L=α

**Parameters of Clay Codes Evaluated**

![image-20211114154503959](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114154503959.png)

通过增加d-k+1，减小修复带宽

例子（n=4,k=2） d = 3, α = 4,β=2,M=8 

存储开销n/k=2,修复带宽降到了0.75

**Starting Point**

> (4,2)Scalar RS Code

![image-20211114160815149](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114160815149.png)

有相同的y，称之为由相同的y-coordinate

**The Uncoupled Code**

存储在相同的4个节点上，相同的RS码，4个码字

![image-20211114161908839](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114161908839.png)

每个节点存储4个字节，每个字节与一个不同的码字相关联，用z来索引

由4个垂直的列组成，每个列由4个柱面组成，每一个列存储一个超字节，每个柱面存储一个字节

**Using a Pair of Coordinates to Represent a Layer**

层的耦合用层的z索引来表示 z=2z<sub>0</sub>+z<sub>1</sub>

用x=z<sub>y</sub>

![image-20211114163713876](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114163713876.png)

**Pairing of V ertices and Bytes**

用p来代替（x,y,z）,p和p*耦合

被染成红色的是未配对的，剩余的顶点成对，有相同的y

16个顶点中，由8个是未配对的

图7中黄色线连接起来是配对的

![image-20211114165237884](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114165237884.png)

**Transforming from Uncoupled to Coupled-Layer Code**

![image-20211114183651181](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114183651181.png)

**Encoding the Clay code**

具体过程再看看

**Intersection Score**(IS)

## 4. Ceph and Vector MDS Codes

### 4.1 Introduction to Ceph

Ceph

> 分布式存储系统，将数据存储为对象
>
> OSD
>
> > 守护进程，与存储单元(如固态或硬盘)相关联，用于存储用户数据
>
> 支持多个EC，对象存储在逻辑分区被称之为pool
>
> 每个pool有多个PG，一个PG是n个OSDs,n是与池关联的纠删码块长度

OSDs分配是动态的，由CRUSH算法执行。

> 当一个对象流到Ceph时，分配一个PG，每当有新对象添加时，或者active osds失效时，会进行动态负载均衡
>
> 有一个OSD被指定为主OSD（p-OSD）,需要存储一个对象时，该对象被传递给已分配PG的p-OSD，该p-OSD还负责发起编码和恢复操作

从data object到数据块的传递时两步完成的。对于大对象，执行编码和解码操作所需的缓冲内存量会很大。先被划分为称之为stripes，大小用S表示，不够补0

### 4.2 Sub-Chunking through Interleaving

补0，确保S可以被kα整除

编码相当于码字为N=s/kα,下一步，从每个OSD的末尾获得α子块，每个N字节，我们使用之前L的定义，L=S/k

向量码的优点是，通过传递α子块的子集来修复被擦除的编码块，但是会导致分段读取。

### 4.3 Implementation in Ceph

实现了Jerasure和GF-Complete 

实现中，多了一块额外的缓冲区，U-buffer，用来存储U，大小为nL=Sn/k

**Pairwise Transforms**

给了sure_matrix_dotprod()和galois_w08_region_multiply()来实现{U,U<sup>∗</sup>,C,C<sup>∗</sup>},剩下两个块求解另外两个

**Encoding**

对一个对象的编码，通过p-OSD,假设m个校验块已经被erased

然后通过初始化码的解码算法，使用这些数据块恢复m个块。

与MDS编码相比，对正向和反向转换是Clay编码所需要的唯一额外计算

**Enabling Selection Between Repair & Decoding**

当多个OSDs出故障，会影响多个PG，触发所有相关对象的恢复操作，引入is_repair()，以便在带宽、磁盘I/O高效修复算法和默认解码算法之间进行选择

**Helper-Chunk Identification**

mini-mum_to_repair()，选择d个helper

**Fractional Read**

对于有效的修复，我们只读取块的一部分，通过ECSubRead

**Decode and Repair**

### 4.4 Contributions to Ceph

**Enabling vector codes in Ceph**

增加了向量码的插件

**Clay codes in Ceph**

实现了Clay Code

## 5.Experiments and Results

单节点的性能

### 5.1 Overview and Setup

我们对选定的参数集和实际比较相近的实验评价

![image-20211114204013206](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114204013206.png)

C1 VS RDP

C2 VS LRC

C3 VS RS（used in Backblaze ）

C4 C5 C6和（14，10）RS对比，主要是multiple-erasure的情况

**Experimental Setup**

M4.xlarge

> (16GB RAM, 4 CPU cores)
>
> 500G ssd

Ceph storage cluster

> consists of 26 nodes
>
> One server
>
> > dedicated for the MON daemon
>
> remaining 25 nodes
>
> > each run one OSD
>
> 总存储12.2TB

**Overview**

![image-20211114205213813](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114205213813.png)

两种负载，固定和可变

可变

> 64MB
>
> > 82.5%
>
> 32MB
>
> >  10% 
>
> 1MB
>
> > 7.5%

故障域是一个节点，我们通过移除OSDs将节点故障注入系统。

利用nmon和NMONVisualizer工具进行测量

单个PG和多个PG的方案

测量

> repair network traffic
>
> repair disk read
>
> repair time
>
> encoding time
>
>  I/O performance for degraded

### 5.2 Evaluations

**Network Traffic: Single Node Failure**

![image-20211114211057191](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114211057191.png)

![image-20211114211105064](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114211105064.png)

![image-20211114211316617](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114211316617.png)

**Disk Read: Single Node Failure**

非连续读会引起性能下降

最坏和最好的情况分析

![image-20211114211448822](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114211448822.png)

![image-20211114212023158](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114212023158.png)









## 6 Handling Failure of Multiple Nodes



### 6.1 Evaluation of Multiple Erasures

这部分结合附录来看



## 7. Conclusions

Clay code理论上在MDS中由最小可能的修复带宽和磁盘I/O。在MSR码类中，Clay code具有最小可能水平的子分组。我们通过实验验证了这些特性的存在。

Clay code的构造分为两步

> stacks in layers, 从MDS代码中提取的α码字
>
> 将不同层的元素配对并转换以生成Clay代码

实现了从理论到实践的飞跃























