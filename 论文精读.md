# XORing Elephants: Novel Erasure Codes for Big Data

## 0. ABSTRACT

RS基础上做了改进，通过增加存储冗余，优化性能

## 1. INTRODUCTION

介绍了多副本，纠删码。

纠删码的修复主要问题是，带宽开销，假如是（10，4），修复一块需要10倍块大小的带宽

提出了LRC的概念

## 1.1. Importance of Repair

分析了Facebook的相关数据得出降低网络带宽代价是重要的结论

efficiently repairable重要的原因

1. degraded reads

   > 很多瞬时错误并不会丢失永久性数据，但是会造成数据不可用，这个时候读编码的条块会读降级。
   >
   > 这个时候可以通过修复过程重建数据块，但是目的不是容错，而是为了更高的数据可用性。重建的块不必写入磁盘
   >
   > 所以efficient and fast repair 可以提高数据可用性

2. efficient node decommissioning

   >Hadoop可以让故障节点退役
   >
   >Functional data必须在退役之前从节点中复制出来，这是一个复杂且耗时的过程
   >
   >Fast repairs可以把节点退役视为定期修复，并且重新创建块，不会产生非常大的网络流量

3. repair influences the performance of other concurrent MapReduce jobs

   > 因为修复需要占用一定的网络带宽，与数据中心的网络带宽相比，存储空间的增长速度不成比例地快。所以这个问题会越来越严重，所以local repairs显得越发重要

4. local repair would be a key in facilitating geographically distributed file systems across data centers

   >RS在跨地理的程度上是不可行的 因为high bandwidth requirements across wide area networks
   >
   >local repair是可行的

复制可以很好的处理上面的问题，但是存储开销较大。MDS开销小，但是会遇到上面的问题。

本文可以看作是牺牲了一些存储效率，来达到其他指标

## 2. THEORETICAL CONTRIBUTIONS

MDS在通信和存储领域应用非常广泛

MDS是最低恢复冗余

两个定义

1. Minimum Code Distance
2. Block Locality

locality和good distance是矛盾的

LRC(k, n−k, r)

### 2.1. LRC implemented in Xorbas

Ci的选择有要求 线性无关，不能为0

设计了一个随机算法和确定算法，可以生成系数

有个优化S3不用存储，构造出来S1+S2+S3=0

系数的构造

## 3. SYSTEM DESCRIPTION

在HDFS-RAID上实现

> RaidNode 
>
> > 负责创建和维护校验块
>
> BlockFixer
>
> > 用来修复块
>
> ErasureCode
>
> > 实现编码和解码功能，上面两个组件都依赖于他

HDFS-Xorbas 在HDFS-RAID基础上增加了LRC

### 3.1. HDFS-Xorbas

#### 3.1.1. 编码

RaidNode 将一个文件分成10块，然后编码出4块。可能一个文件可能不够10块，默认剩下的填充为0，依旧是10块

![image-20211009150941798](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211009150941798.png)

LRC会额外计算两个块，如上图所示

#### 3.1.1.2 解码和修复

RaidNode

> light-decoder
>
> > 针对于每个条带单个块的错误
>
> heavy-decoder
>
> > light-decoder失败的时候使用

BlockFixer

> 检测到块失败，决定LRC恢复需要的5个块
>
>  light-decoding尝试恢复
>
> 如果出现multiple failures，可能没有需要的5个块， light-decoder失败， heavy-decoder启动
>
> heavy-decoder跟RS恢复过程一样，将结果发送和存储到数据节点

##  4.RELIABILITY ANALYSIS

 mean-time to data loss (MTTDL) 可靠性分析的依据

> 可以容忍的故障数
>
> 修复的速度
>
> 弹性增加和修复时间减小，MTTDL也会增加

结果LRC，RS比复制高了很多，但是复制的数据可用性比LRC和RS强

## 5. EVALUATION

两种环境性能

Amazon’s Elastic Compute Cloud

 a test cluster in Facebook

### 5.1 评价指标

HDFS Bytes Read

> 对应于为修复而启动的作业所读取的总数据量
>
> collected from the statistics-reports of the jobs spawned following a failure event

Network Traffic

> the total amount of data communicated from nodes in the cluster
>
> 单位为GB
>
> 用下面这个工具来监测
>
> Amazon’s A WS Cloudwatch monitoring tools

Repair Duration

>  the time interval between the starting time of the first repair job and the ending time of the last repair job.

### 5.2 EC2

two Hadoop clusters

> HDFS-Xorbas
>
> HDFS-RS

Each cluster

>  51 instances of type m1.small
>
> 1 master  hosting Hadoop’s NameNode, JobTracker and RaidNode daemons
>
> 50  slave as DataNode and a TaskTracker daemon

file size 640 MB

block size 64MB

每个文件两个集群中分别生成14和16块

故障包括一个数据节点或者多个数据节点的终止

四个故障事件是单节点错误，两个三节点错误，两个两节点错误

文件数量（20，100，200）

#### 5.2.1 HDFS Bytes Read

HDFS-Xorbas 比HDFS-RS好41%-52%

读的平均块数从11.5降到5.8

#### 5.2.2 Network Traffic

网络流量和读取的字节数基本上一致，二倍的关系

#### 5.2.3 Repair Time

Xorbas比HDFS-RS快25%到45%

实验里面带宽没满，实际环境中带宽可能跑满，时间表现可能更好

#### 5.2.4 Repair under Workload

为了演示修复性能对集群负载的影响。

创建了两个集群

每个集群15个从节点

块故障时不可用，LRC相比RS延迟小

### 5.3 Facebook’s cluster

区别点在于利用的集群中现有的数据集

块大小为256MB

94%文件3块 剩下10块 平均3.4块

由于块大小比较小，Xorbas比HDFS-RS存储开销大了27%（最好应为13%）

## 6. RELATED WORK

functional repair

> 虽然块可以恢复，但是这个时候确实不可使用，需要其他k块来恢复

exact repair

使用更小网络代价来修复是有可能的

> low rate
>
> high rate

这部分涉及到的工作还挺多的，有时间可以看一看具体内容

## 7. CONCLUSIONS

LRC降低带宽开销2倍，增加存储开销14%

提出了想法，应用在宽条带上，RS在宽条带上不可行，因为带宽要求随着块大小增长



# StripeMerge: Efficient Wide-Stripe Generation for Large-Scale Erasure-Coded Storage

## 0.Abstract

极限存储-宽条带 带宽开销巨大

StripeMerge 宽条带生成机制

将窄条带变成宽条带，这个过程中是为了最小化宽条带生成带宽

本文工作证明了最优方案的存在，还有两个启发式算法，生成时间减小87.8%

## 1.INTRODUCTION

为了保证data durability提出复制，但是冗余开销太大

RS是一种替代方案，冗余开销很低，但是现在还在追求更低的存储开销

宽条带

> 极端的存储冗余
>
>  more on low-cost storage durability than high data access performance
>
> 重构的时候，带宽消耗很严重

应该根据年龄不同，做不同的参数化，分层处理

> 数据块在新写入时往往被访问得更频繁，但随着它们的年龄变得更少
>
> 随着数据的老化，窄条带被重新编码为宽条带

将窄条带重新编码为宽条带不可避免地会重新定位数据块并重新生成校验块，从而导致数据传输中巨大的带宽开销

主要观点

宽条带的生成发生在有大量节点和条带的大型存储系统

选择两个窄条带合并成一个

 StripeMerge

## 2.BACKGROUND ANDMOTIVATION

### A Erasure Coding

讲了纠删码的概念，MDS码

### B Wide-Stripe Generation

宽条带的概念

假设

> 宽条带用于很少被访问的冷数据，比如备份和归档数据[1]、[6]或二进制大对象(blob)，它们的访问频率随着时间的推移而下降

宽条带有较高的修复代价，随着k增大而增大

宽条带生成问题

>宽条带生成步骤（两个窄条带生成一个宽条带）
>
>>重新分配2k块，分布在不同的节点
>>
>>将两个窄条带的一些数据和校验块迁移到一些负责生成宽条带的新校验块的节点上，它们的奇偶校验块稍后分布在不同的节点上。
>
>数据块的重新分布，从窄条带重新生成校验块，存在明显的数据传输
>
>目标最小化带宽

### C Storage Scaling

增加新节点扩容，为了在现有和新增加的节点上重新分配擦除编码块，他们研究了如何将(k,m)条转换为(k+s,m)条，以最小化伸缩带宽(即伸缩期间传输的数据量)

NCScale的扩展方法

![image-20211014093039882](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014093039882.png)

此时宽条带生成带宽依旧不为0，宽条带与存储扩展有所不同的是它不需要增加新的节点，这种差异导致它有不同的解决方案。

### D Our Idea

Perfect merging

> 从当前存储的大量窄条带中，选择两条合适的窄条带，用来合并为宽条带，不需要生成带宽

例子如下

![image-20211014095141389](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014095141389.png)

b和a的区别在于c ，d在单独的节点，并不需要再迁移，这个地方不消耗带宽

图示的两个条带有这样的特点，用来消除宽条带生成带宽

> 数据块在不同的节点
>
> > 因为宽条带最后数据块分布在不同的节点，如果有在相同节点的，那么会导致数据块迁移到其他节点，有带宽开销
>
> 校验块有相同的编码系数，并驻留在相同的节点，这些可以用来生成新的校验块

例子如下

![image-20211014095936799](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014095936799.png)

![image-20211014095943051](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014095943051.png)

由上面两个式子可以看出来，新的校验块可以通过之前的校验块本地生成，不产生带宽消耗

![image-20211014100208111](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014100208111.png)

**Challenges**：Perfect merging不产生带宽开销，但是如何在系统中应用Perfect merging生成多个宽条带。并且Perfect merging跟窄条带的放置方式有关。大规模存储系统中，搜索所有的窄条带会消耗大量时间。

## 3.ANALYSIS

SectionIII-A 我们将宽条带生成问题转换为bipartite graph model

SectionIII-B 证明在给定足够多的窄条带的情况下，总是存在一个可以生成所有宽条带而不需要任何宽条带生成带宽的最优方案。

### A Bipartite Graph Model

n个节点，足够多的(k,m)narrow stripes，目标是选择所有符合Perfect merging的窄条带，合并成(2k,m)的宽条带，这样就不存在跨节点的生成带宽

（k,m）条带，随机分布在k+m节点上

![image-20211014102642933](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014102642933.png)

总共排列方案，假设是每个都不同。但是实际上我们合并的时候，数据块顺序无所谓（我们计算的时候只用了校验块），校验块的顺序是重要的，对计算产生影响（主要是前面的系数）

![image-20211014104058750](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014104058750.png)

这种被认为是同一种摆放方式。

![image-20211014104323104](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014104323104.png)

所以总共可能性如上，被称为complete chunk placement set。

![image-20211014104423710](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211014104423710.png)

提出了 bipartite graph

为什么x和y之间的节点连接起来就是完美匹配？

### B Existence

证明perfect merging的存在

证明没看懂

## 4.STRIPEMERGE

### A Limitations of Optimal Scheme

带宽问题，转化为二部图的最大匹配问题

最大匹配算法时间复杂度高，形成一个二部图在时间和空间上都是昂贵的，并且实际中，块数不一定一样，不一定能形成完美匹配。

### B Greedy Heuristic: StripeMerge-G

利用现有窄条纹的完美合并来生成尽可能多的宽条纹，而对于剩余的窄条纹，我们转移一些块使它们满足完美合并来生成剩余的宽条纹

merging cost 

> 转移块的数量

例子

![image-20211017124421315](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211017124421315.png)

设计算法：

![image-20211017124940726](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211017124940726.png)

思路

> stripemerg首先计算任意一对窄条纹的合并代价，并构造一个包含所有窄条纹对及其合并代价的集合(第1-7行)
>
> 根据代价排序，代价为0到k+m之间的整数，计数排序时间复杂度O(n<sup>2</sup>),n为窄条带数量，那么一共n(n-1)/2对，选择最小的合并，然后删除相关元素
>
> 时间复杂度为O((k+m)n<sup>2</sup>)并没有显著降低（主要是遍历求代价这部分）

### C Parity-aligned Heuristic: StripeMerge-P

完美合并的条带有以下特点

parity-aligned

> parity chunks have identical encoding coefficients and reside in identical nodes

目标是识别完全奇偶对齐的窄条纹对，从而快速获得满足完美合并的窄条纹对，显著减少了作为算法1输入的条纹数量，还识别partially parity-alignedpairs的条带块

构造一个集合校验块对齐的数量的集合，为了构造这个，将校验块的位置的元数据存储在哈希表中。

哈希表存储key-value，key指向奇偶块的具体拜访，value是具有相应奇偶块位置的条带的索引列表。O(1)时间查找到具有相同校验块位置的条带

对于任意条带，其校验块放置在m节点中，它可以为其所有奇偶校验块放置生成2<sup>m</sup> - 1个不同的键,这些会造成二外的内存开销，但是这个开销是有限的。

在StripeMerge-G基础上做了修改

具体算法

![image-20211017141058543](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211017141058543.png)



这种方法思想是找校验块对齐的条带，进行合并。但是他们的数据块可能在同一个节点，这样会造成数据迁移，cost增加

## 5.PERFORMANCE EVALUATION

 Amazon EC2

StripeMerge VS NCScale

比较点，节省多少带宽

 可以减少多少运行时间

### A Simulations

not implement coding operations and data transfers（没有数据传输这个带宽咋来的？？？）

> Intel Xeon Silver 4110 2.10 GHz CPU
>
>  256 GiB RAM
>
>  ST1000DM003 7200 RPM 1 TiB SATA hard disk
>
> 设N为2k+m的倍数

#### Experiment A.1 (Wide-stripe generation bandwidth)

10000个条带随机分布在N个节点，测试不同的N和（k，m）的组合

> StripeMerge-P ,StripeMerge-G, NCScale
>
> 4≤k≤64,2≤m≤4 N=2(2k+m) and N=4(2k+m)

![image-20211015162954707](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015162954707.png)

从图中可以看出来，带宽随着k和m的增加而增加，因为会使得完美匹配更加难满足

当k比较小时生成带宽随着N的增大而增大，k比较大时，随着N的增大而减小。

正面影响，N越大数据块越容易分布在不同的节点上，负面影响校验块越不容易驻留在相同节点。

k不大时，m和k差不都，负面影响大。k较大时，k远大于m，正面影响大，带宽减小。所以更大的k和N会受益（觉得缺乏说服力）

StripeMerge-P 和 StripeMerge-G性能差不多

#### Experiment A.2 (Running time versus(k,m))

StripeMerge-G and StripeMerge-P的运行时间对比（为啥不跟NCScale比，这个时间是0么）

![image-20211015164744703](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015164744703.png)

StripeMerge-P比StripeMerge-G 快，说明parity-aligned有效的加快了速度，k较小时，基本上是0.

优化幅度随着k的增大而减小，k越大，更多的数据块有糟糕的数据块布局，parity-aligned不能加速算法。

#### Experiment A.3 (Running time versus the number of narrow stripes)

测试算法运行时间与窄条纹数量

固定(k,m) = (16,4)andN=2(2k+m) =72

StripeMerge-G 随着条带数量显著增加（O((k+m)n<sup>2</sup> )），StripeMerge-P线性增加(O((k+m)mn))更适合大型系统

![image-20211015165442925](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015165442925.png)

#### Experiment A.4 (Memory consumption of StripeMerge-P)

total memory usage 

the memory usage of the hash table that stores parity-aligned metadata

fix(k,m) = (16,4) and N=2(2k+m) =72

![image-20211015181221294](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015181221294.png)

(注意轴是指数型的)，由此可见哈希表的开销和总内存开销相比是有限的，处理10000条窄条带需要4.85GB的内存，哈希表本身只需要72.5MB的内存，StripeMerge-P产生的额外开销是可以接受的。

### B Amazon EC2 Experiments

扩展了coding and data transfer功能，使用ISA-L实现编解码

N=2(2k+m) 个 m5.xlarge 实例 当存储节点，为了评估网络带宽的影响，专门配置了一个用作网关的专用实例。![image-20211015184350258](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015184350258.png)

来自实例的任何传输块都必须在到达另一个实例之前遍历网关。我们使用Linux流量控制命令来控制网关的输出带宽

实验中，将带宽从1Gb/s到8Gb/s变化，实验考虑不同的块大小 10,000 narrow

#### Experiment B.1 (Time breakdown)

时间分为3部分

1. 算法运行时间，找到需要合并的窄条带
2. 转移时间，指的是用于合并的块的转移
3. 计算时间，是指将窄条带的校验块合并为宽条带的新校验块的本地计算

> (k,m) = (16,4),N=2(2k+m) =72 
>
> chunk size of 64MiB 
>
> gateway bandwidth of 8Gb/s

![image-20211015190447156](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015190447156.png)

由表格可知传输时间占主导地位

stripemerg的运行时间占用了10,000条条纹总时间的1.65%，但这个百分比将随着条纹数量的增加而急剧增加(实验A.3)。因此，对于具有大量分条的大型存储系统，stripemerg的运行时间会降低整体性能。相比之下，stripemerg  - p的运行时间仅占10,000条带总时间的0.068%，而这一百分比只随着条带数量的增加而线性增加

#### Experiment B.2 (Wide-stripe generation time versus (k,m))

![image-20211015191921094](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015191921094.png)

gateway bandwidth as 8 Gb/s and the chunk size as 64 MiB

 k m 增加生成时间增加 ，完美匹配的少了，需要传输的数据多了，时间会变长

#### Experiment B.3 (Impact of gateway bandwidth)

(k,m) = (16,4),N=2(2k+m) =72

 gateway bandwidth, from 1 Gb/s to 8 Gb/s

![image-20211015192947798](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015192947798.png)

生成时间随着带宽增大而线性减小，并且StripeMerge 优于NCScale

#### Experiment B.4 (Impact of chunk size)

 chunk sizes, from 8 MiB to 64 MiB

(k,m) = (16,4),N=2(2k+m) =72 

gateway bandwidth of 8 Gb/s

 ![image-20211015193338865](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211015193338865.png)

生成时间随着块大小线性增加，而且StripeMerge性能仍然较好

## 6.RELATEDWORK

在repair和scaling roblem最小化带宽的研究

repair

>locally repairable codes (LRC)
>
>> 通过额外的存储减小修复I/O
>
>regenerating codes(再生码)
>
>> 通过额外的存储减小修复I/O
>
>repair-efficient techniques
>
>> lazy recovery通过谨慎地延迟立即修复操作来减少修复流量
>
>parallelizing and pipelining repair
>
>> 减少修复时间
>
> scheduling repair tasks in free timeslots
>
>> 适应工作负载的动态变化

scaling problem

> minimize the bandwidth
>
> >  under RAID-0, RAID-5 (i.e., single fault tolerance) , or RS codes
>
> code conversion
>
> > 研究可转换的代码结构，以最小化代码转换中的I/O

本文研究的问题其实和上面的都不一样，研究如何最小化带宽并且减少宽条带生成问题的计算开销。

宽条带问题的相关研究

> VAST
>
> > locally decodable codes提高宽条带的修复性能
>
> Haddock et 
>
> > general-purpose GPUs 提高宽条带的解码效率
>
> ECWide
>
> > combined locality
> >
> > 系统的解决了修复带宽问题，提出了有效的编码和更新方案

本文关注点在于如何生成宽条带

## 7.CONCLUSIONS

StripeMerge生成宽条带

本文转换为bipartite graph modeling，证明了最优方案的存在。提出了两个算法，在有限带宽下生成（完美情况时间复杂度比较大），生成性能比现有性能好。

## 修正

文章中提到的性质，是直接使用范德蒙矩阵的子矩阵，但是在k，m较大时，这种矩阵不一定有逆，所以业界现在用的范德蒙矩阵，上面化成单位阵，下面系数，已经不存在本文中的性质了。所以文章的理论出发点存疑，不过m=2时，感觉是成立的，但是文章是对任意m的。验证刚才提到的内容可以通过Jerasure输出编码矩阵，这个事情在Note Correction to the 1997 Tutorial on Reed-Solomon Coding有提到。

## 教训

尽信书不如无书，对于细节部分一定要研究仔细

# Erasure Coding in Windows Azure Storage

## 0. Abstract

Windows Azure Storage (WAS)使用纠删码，引入了LRC的概念，减少修复所需要的块数，从而减小带宽和I/O 

## 1. Introduction

 (WAS)的一个简介

 stream layer

> append-only distributed file system

active-extents

>  replicated three times by the underlying stream layer. 

先写入三副本，达到一定大小，区域会被sealed，不能再被修改，成为纠删码的候选区，开始编码，编码结束，删除3副本

使用纠删码可以降低成本50%以上，存储马上达到EB，节省更多硬件，节省数据中心占地面积，节省电力。

trade-off是性能

dealing with

> a lost or offline data fragment
>
> hot storage nodes

数据不可用两种情况

> 丢失
>
> 所需数据在升级的节点上

重构返回数据给客户端，需要优化，优化方向

> networking bandwidth
>
> I/Os 
>
> 重构时间

纠删码，数据块存储在特定的节点上，增加了存储节点变热的风险，影响时延（疑问？有一篇说分片可以有效地减少负载不均衡）

WAS处理方法，识别热片段，存储到较冷的节点从而负载均衡，或者cache缓存数据，提供服务。

但是这样在完成迁移和cache缓存前，会影响性能，优化，如果读太久，直接重构。

所以对重构时间有要求，时间取决于最慢的节点

（12，4），重构成本大

减小读取片段数的好处

> reduces the network overhead and number of I/Os
>
> reduces the time it takes

引入了LRC实现（关键点在于98%的故障时单点故障，所以LRC很有用）

## 2. Local Reconstruction Codes

### 2.1 Definition

举了个（6.3）的例子

![image-20211020111728150](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020111728150.png)

LRC(k,l,r)

### 2.2 Fault Tolerance

容错的例子

构造特定系数实现Maximally Recoverable(MR) property

![image-20211020112525634](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020112525634.png)

#### 2.2.1 Constructing Coding Equations

![image-20211020112611275](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020112611275.png)

![image-20211020163149256](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020163149256.png)

这种情况下可以容任意3错，问题是如何求系数

关注以下情况

> None of the four parities fails
>
> Only one of p<sub>x</sub> and p<sub>y</sub> fails
>
> both p<sub>x</sub> and p<sub>y</sub> fails

这三种情况是可以解码的（具体内容可以之后再看看）

#### 2.2.2 Putting Things Together

可以对3中4错情况进行解码，占所有4故障的86%，实现了最大可恢复属性

#### 2.2.3 Checking Decodability

如何判断是否可恢复

对于每个本地组，如果校验块是可用的可以当作一个缺失的数据块，校验块标记为删除，完成本地组后，检查数据片段和全局校验块。如果缺失的数据片段数不超过全局校验块的数量，则理论上是可解的。否则不可解码。

### 2.3 Optimizing Storage Cost, Reliability and Performance

LRC(k, l, r)

> 单故障错误，修复需要k/l片段
>
> 可以容r+1错

n-k >= l+r

### 2.4 Summary

(k,r,l)容错在r+1 - r+l（能容多少错，取决于具体的故障情况）

## 3. Reliability Model and Code Selection

LRC的参数选择问题，可靠性要达到三副本的可靠性

### 3.1 Reliability Model

利用Markov models来进行判断

#### 3.1.1 Markov models

![image-20211020172107114](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211020172107114.png)![image-20211024160740251](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211024160740251.png)

利用马尔可夫链求出了MTTF

比三副本和（6，3）都要高，因为容任意三错，并且可以容86%的4错

### 3.2 Cost and Performance Trade-offs

![image-20211024161613591](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211024161613591.png)

每个点代表一组参数

下界，代表相同存储冗余。重建开销较小。

### 3.3 Code Parameter Selection



![image-20211024162130837](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211024162130837.png)

### 3.4 Comparison - Modern Storage Codes

Weaver codes

HoVer codes

Stepped Combination codes

这三种编码回头可以看看

![image-20211024163631562](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211024163631562.png)

性能好处在于

> 校验分为本地和全局的
>
> > 本地校验只涉及最小的数据片段
> >
> > 全局涉及所有数据片段，可以提供多个容错性

其他编码都是相同的责任，重构和容错

### 3.5 Correlated Failures

马尔可夫链 要求是独立的，所以我们分别放置在不同的地方。如果有例外的情况要在链上添加弧。

## 4. Erasure Coding Implementation in WAS

分为三层

> front-end layer
>
> partitioned object layer
> stream replication layer（EC实现）这个地方有个东西需要再看看

### 4.1 Stream Layer Architecture

Stream Managers

![image-20211023153158529](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211023153158529.png)

Streams

> save a list of **extents**
>
> >  a list of append blocks
> >
> > > each block CRC(循环冗余校验（Cyclic Redundancy Check， *CRC*）)
> >
> > extent is replicated on multiple (usually three) ENs
>
>  Write operations for a stream keep appending to an extent
>
> > the extent reaches its maximum size (in the range of 1GB-3GB)
> >
> > there is a failure in the replica set

### 4.2 Erasure Coding in the Stream Layer

erasure coding process

> completely asynchronous
>
> off the critical path of client writes

1.  SM creates fragments on a set of ENs
2. SM designates one of the ENs in the extent’s replica set as the coordinator of erasure coding
3. sends it the metadata for the replica set

![image-20211023194815165](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211023194815165.png)

### 4.3 Using Local Reconstruction Codes in Windows Azure Storage
 placement of the fragments

> load 为了负载均衡
>
> reliability 尽量不放在相同域

涉及到一个升级域的概念 需要再研究下

### 4.4 Designing for Erasure Coding

**Scheduling of Various I/O Types**

stream layer handles a large mix of I/O types

> open/close, read, and append operations from clients, create, delete, replicate, reconstruct, scrub, and move operations generated by the system itself

这么多操作，存在调度问题

**Reconstruction Read-ahead and Caching**

等不可用片段达到一定大小再开始重建，减少磁盘和I/O的数量

**Consistency of Coded Data**

检查数据一致性

Checksum and parity是防止数据损坏的两个主要机制

## 5. Performance

与RS码在大小两种I/O下进行比较

LRC（12，2，2）和RS(12,4)进行比较

（12，3）的可靠性低于三副本

### 5.1 Small I/Os

key metric(4KB to64KB range)

> latency
>
>  the number of I/Os taken

负载较轻时，没有太大区别。

负载较重时，读取很多块，延迟取决于最慢的片段，选择更多的数据片段。有效降低延迟。

LRC读的块数少延迟低，虽然比更多的片段延迟大，但是带宽节省更多

![image-20211022190238660](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211022190238660.png)

### 5.2 Large I/Os

4MB large I/Os(4MB large I/Os)

>  latency
>
> bandwidth consumption

负载较轻时，时延仍然增大

延迟在网络和磁盘带宽

 1Gbps network

LRC由于减少了数据传输量，性能提高，片段越多反而性能越差



![image-20211022193653754](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211022193653754.png)

### 5.3 Decoding Latency

LRC比RS解码快，但是解码比传输小几个数量级，并不重要。



## 6. Related Work

Erasure Coding in Storage Systems

> 很多系统使用了纠删码

Performance

> 在擦码存储系统中，节点故障会触发重构进程，导致重构读取时的延迟性能下降。此外，经验表明，没有数据丢失的瞬时错误占数据中心故障的90%以上
>
> 重构会导致读性能下降

Erasure Code Design

> 这部分没看懂，需要一会再看看

## 7. Summary

引入LRC 从（12，4）改为（12，2，2），比传统的三副本持久性好（应该没有（12，4）好）

# Network Coding for Distributed Storage Systems

## 0.abstract

RS码通过更少的冗余达到相同的可靠性水平，但是网络传输量大。本文提出了再生码的概念，可以显著的降低带宽。

storage and repair bandwidth有一个权衡，本文对他进行了描述

## 1.introduction

分布式存储系统通过分布式的存储节点来长期可靠地存储数据，单个节点存储是不可靠的。

一些相关的存储系统

> OceanStore
>
> Total Recall 
>
> DHash++ 

提供可靠性，都需要增加冗余，复制是一种方法，RS冗余更小。相同冗余下，可靠性比复制高几个数量级

节点故障或者离开系统，冗余不断刷新，涉及到网络中大量的数据传输

![image-20211028112546349](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211028112546349.png)

生成一个M/K的片段，传输量需要M，事实上所有的已知编码结构都需要访问原始数据对象来生成编码片段

本文中，展示了修复可以不与整个对象通信。

2MB对象，（4，2）可以通过1.5MB进行修复， information theoretic minimum

确定了tradeoff在存储和修复带宽上，提出了再生码的概念，纠正了之前文献中的一个计算错误

两个极值点MDS,MBR

MBR有最小的修复带宽如果每个节点存储的多余M/k，修复带宽会显著下降

讲解了下本文的组织结构

## 2. BACKGROUND ANDRELA TEDWORK

### A.Erasure Codes

 tradeoff

> redundancy
>
> error tolerance

在冗余和可靠性权衡下，RS是最优的。

近期的一些研究关注在其他指标上

> sparse graph codes 
>
> > 性能接近最有，编码和解码复杂度较低
>
> parity array codes
>
> > 基于异或操作，目标是低解码，编码，更新复杂度。

本文的侧重点，指标上增加了带宽的指标。

### B.Network Coding

网络编码是传统路由(存储转发)方法的推广

允许中间节点通过编码，来生成输出数据，允许数据在中间节点混合。

本文考虑了网络编码在分布式修复问题中的应用，考虑修复带宽和存储之间的权衡。

### C.Distributed Storage Systems

之前的一些工作，对比了复制和RS的性能

Hybrid strategy

> 一个特殊节点除了维护多个被擦除的片段，还维护一个完整的副本
>
> 这个节点可以产生新的片段，并且发送给新来者
>
> 但是会降低带宽效率，并且使系统设计变复杂

RS存储效益好，但是带宽成本太高

混合策略不现实

## 3.ANALYSIS

我们的分析基于信息流图G，该图描述了数据对象的信息如何通过网络进行通信，存储在内存有限的节点中，并到达数据收集器的重构点

### A. Information Flow Graph

有向无环图

> 源数据节点 S
>
> 存储节点 x<sup>i</sup><sub>in</sub>, x<sup>i</sup><sub>out</sub>
>
> 数据收集者DC<sub>i</sub>
>
> 系统中的存储节点由 x<sup>i</sup><sub>in</sub>, x<sup>i</sup><sub>out</sub>表示，这两个节点由一条有向边连接，容量等于存储在节点上的数据量。
>
> 节点要么是活动的，要么是不活动的，取决于是否可用

具体变化

> 初始时，只有源节点是活动的，然后它接触一组初始的存储节点，用有向边连接到 x<sup>i</sup><sub>in</sub>，此时源节点变得不活动
>
> 存储节点变得活跃，代表一个分布式EC。新节点加入系统只能与活动节点相连。
>
> minimum cuts
>
> > 

![image-20211031183002355](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211031183002355.png)



### B. Storage-Bandwidth Tradeoff

n节点存储a个字节，一个节点失败，一个新节点会跟d个节点每个通信B字节，总修复带宽是y=dB

（n,k,d,a,y）

中间有个很复杂的证明过程



### C. Special Cases: Minimum-Storage Regenerating (MSR) Codes and Minimum-Bandwidth Regenerating (MBR) Codes


## 5.CONCLUSION

本文提出了一个理论框架，可以确定在编码系统中修复必须要通信的信息，并确定存储和修复带宽之间的权衡。

未来计划

> investigate deterministic designs of regenerating codes over small finite fields, the existence of systematic regenerating codes
>
> designs that minimize the overhead storage of the coefficients, as well as the impact of node dynamics in reliability

再生码，一个潜在应用领域是分布式归档存储或备份，其中的文件通常较大且不常被读取。

在这种情况下，再生码会在冗余、可靠性和修复带宽方面提供理想的折衷



## 6.附录



# Geometric Partitioning: Explore the Boundary of Optimal Erasure Code Repair
## 0.Abstract

再生码是特殊的纠删码，目的是减少修复所需的数据量

> 再生码的修复粒度是块而不是字节（纠删码都是按照块修的吧？）
>
> 块大小的选择会导致流降级读时间和修复吞吐量之间的紧张关系（关键在块大小）

文章为了解决这个问题提出了Geometric Partitioning，将每个对象按照其大小和geometric顺序分成一系列块，来获得大块和小块的好处（那是不是同时存在大块和小块的坏处，所以要想办法规避这个问题？），可以达到1.85x RS的性能，同时保持较低的读取时间。

## 1.Introduction

提了一些经典的对象存储系统

给了个概念修复成本：由于数据丢失而需要修复的数据量

引出了再生码，修复成本最低

Recovery efficiency

> 系统以多快的速度恢复其原来的容错能力

**修复成本的最优化并不一定能提高恢复效率或减少降级的读取时间**

> 为了修复一个块，只从相应的磁盘中读取一小部分子块。这种分散的磁盘访问模式会导致碎片化，从而降低磁盘性能，从而降低恢复效率

分块修复粒度还会增加降级的读时间，RS可以并行修复，再生码需要等待第一个块修复，可能需要与修复对象一样多的时间

当读取小于chunk大小的对象时，也会修复不必要的数据，导致读放大，降低读时间

再生码，对象存储系统通常选择单个块大小作为编码单元，编码前将对象分割为块。

> 较大的块大小有助于减少磁盘访问时的碎片和不连续读取，从而提高恢复效率。
>
> 但是较大的块大小会增加等待第一个修复块的时间，增加读放大的机会

低退化读时间和高恢复效率可以同时实现

> 关键是在每个对象中使用可变的块大小
>
> 我们从小块开始修复，以避免不必要的等待第一个块的修复，然后限制相邻块的大小比例，使当前块的修复可以早于前一个块的转移

Geometric Partitioning

> 它将每个对象按照几何顺序(例如:例如4MB、8MB、16MB、32MB、64MB等)，并将来自不同对象的块分组到桶中进行编码
>
> 比最小块大小更小的对象，使用RS编码，来消除读放大

## 2.Background

### 2.1 Repair, Degraded Read and Recovery

修复

> 降级读
>
> > 时间是重要指标，修复和传输可以pipelined
> >
> > 降级读时间为传输时间加上第一个块的修复时间
> >
> > 有效的修复可以降低MTTL，增加系统的耐久性，减少降级读的次数减轻系统的负担。
>
> 出现故障
>
> > 恢复效率由吞吐量决定，磁盘带宽成为了恢复吞吐量的决定性因素，因为磁盘带宽更难以充分利用。

### 2.2 Repair for Different Codes

RS的修复过程

**单点故障占98%以上**

LRC修复过程

> 通过连接到更少的节点来减少I/O

再生码的修复过程

> 从d个节点读取，再生码通过引入粒度更细的子块和它们之间更复杂的连接来减少I/O
>
> α和β越小越好，更好的局部性，α=16，β=4最坏情况下会有16个不连续的子块
>
> 讲了一个读取的实例

再生码种类很多，MSR,MBR，Hitchhiker，Simple Regenerating codes

再生码和RS的存储可靠性相同，提供了最佳的维修成本

本文使用clay code,但是方法适用于所有的再生码

> 灵活的编码参数设置
>
> 更低的计算复杂度



几种方法的比较

![image-20211120211404282](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211120211404282.png)







![image-20211120183745107](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211120183745107.png)

## 3.Challenges for Applying Regenerating Codes

### 3.1 The Effect of Chunk Size

再生码和sub-chunking技术混用，减少磁盘I/O。数据被重新编码成块，并利用一个块的大小进行解码，而不会引起像RS代码那种的读放大

**Large Chunk Size Benefits Recovery**

读放大的现象

修复需要64个子块，16次不连续读，对象大小256KB，每块4KB，1MB以上将不会出现读放大。HDD，I/O，4MB（对应块256MB）甚至8MB，以分摊I/O延迟并更好的利用磁盘带宽。

（从这里面看，块越大对磁盘更充分利用）

**Overly Large Chunk Size Harms Degraded Read**

块大小大于对象大小会导致读放大，增加降级读时间。对象大小从KB-GB。

如果一个块256MB大小，可能修复256MB对象，只为读取该块中一个64MB对象，这样会造成额外的磁盘读请求

**Small Chunk Size Benefits Degraded Read.**

> 两个步骤
>
> > 从存储服务器修复必要的数据
> >
> > 将修复后的对象从服务器传输给最终用户

![image-20211121125516784](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211121125516784.png)

再生码块比较大的时候，没办法pipelined，小块可以减少阻塞时间，并使降级的读受益

![image-20211121130218622](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211121130218622.png)

块大小是一个关键因素。较大的块大小可以提高恢复效率，但代价是更严重的读放大和增加的阻塞时间，从而导致更长的读时间下降，反之亦然

### 3.2 Case Study: Applying Regenerating Code into Existing Data Layouts

**Contiguous Layout**

多个对象打包，形成大小相同的大文件，作为一个整体编码。

将再生码应用到连续布局中，可以选择任意大的块大小来实现高效的恢复

> 读放大，对象与块不是对齐的，可能多个对象打包到一个块中
>
> 小对象降级读，可能扩展到整个块或多个块，从而导致降级读时间增加

**Stripe Layout**

将每个对象分割成小的条带，并将每个条带扩展成为多个节点，生成校验块，会导致读放大

![image-20211121134216956](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211121134216956.png)

## 4.Geometric Partitioning

固定块大小不能很好地利用再生码，考虑对不同对象使用不同地块大小，但是仍然存在选择大块，流水线效率降低，小块，恢复效率降低

几何分区的设计基于以下原则:如果一个对象可以被划分成大小可变的块，那么我们就可以使用较小的块来通过流水线减少降级的读时间，使用较大的块来实现效

的顺序读，那么我们就可以同时获得小块和大块的好处

![image-20211121135545765](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211121135545765.png)

将一个块划分成多块，每个块对应一个bucket，每个bucket是磁盘上的大文件，包含来自不同对象的大小相同的块。这些桶的大小形成了一个几何序列。来自k+r磁盘的桶是使用再生码一起编码的

s<sub>0</sub>和q，初值和公比

![image-20211121144151913](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211121144151913.png)

R=S mod s<sub>0</sub>

n是被分割的块数

### 4.1 Cut Front to Eliminate Read Amplification

对象对齐的难点在于很难找到足够的相同大小对象，削去一部分，成为s0的倍数，削掉的部分放入单独的bucket中，叫small-size-bucket.

只要s0够大，就容易找到相同大小的块

Small-size-buckets

> 用于存储对象的front portion
>
> 没有特定大小，可以变化，利用RS编码，消除了读放大，降级读的pipeline可以马上开始，从而减少降级读的时间

大于97.7%的容量被大对象占用

![image-20211121145730619](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211121145730619.png)

### 4.2 Partition Objects Geometrically to Benefit Recovery and Degraded Read

![image-20211121145845170](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211121145845170.png)

为了提高恢复效率，应该增加块大小，从理论上讲，对对象进行分区的最佳办法是不进行分区，使块大小最大，但是影响降级读

引入可变块大小

> 从一个小块开始修复
>
> 相邻大小块的比例应该受到限制，以便当前块的修复可以先于之前块的转移
>
> 块尽可能大

块应该指数增长，导致了几何分割的设计

举了个32MB的例子

几何分割相对于算术序列和常数序列的好处

可以增加平均块大小

### 4.3 Help Pipelining by 2-pass Scan

为了pipelining，限制系数不能非0

算法1确定系数，扫描两次几何序列

> 第一次扫描减去每个bucket的大小，直到剩余的大小太小而不能装入一个bucket。
>
> 随后的扫描使用贪婪策略，尝试选择最大的块大小，直到没有桶可以被填满

### 4.4 Parameter Setting

s0较大可以扩大块大小，增加平均磁盘读写带宽，RS增加的I/O和磁盘流量，增加pipelined的开销

q较小有利于pipelined，但是并不利于恢复效率，对目标工作进行采样来调整s0和q,在降级读和恢复效率之间实现性能平衡，Grid search

## 5.System Design and Implementation

### 5.1 System Design

**Architecture**

RC-Stor

> Directory Server
>
> > 存储元数据并监控整个系统
>
> Storage Server
>
> > 存储和管理对象，存储索引来跟踪对象
> >
> > 绑在一个盘上
>
> HTTP Server
>
> > 处理用户请求

恢复任务由Directory Server分配到Storage Server，Directory Server的可靠性用复制来保证

**Placement Groups**

thousands of PGs

each PG is a group ofk+r=14 disks on 14 different nodes

PG存储在Directory Server上，每个磁盘属于多个PG，每个PG可以独立恢复

当一个磁盘失败，所有的相关PG都开始恢复，将会有更多的磁盘来加速恢复，而不止13个

Directory Server将磁盘分配给PG，以便在磁盘故障时将最大数量的磁盘关联到恢复中

对象首先根据其ID的哈希值映射到PG。然后，选择该PG使用的容量最小的磁盘来放置对象。最后，将对象划分为块并放入相应的bucket中。

**Metadata Management**

对于每个PG，随机选择r+1个磁盘来存储该PG的复制索引（为什么是r+1），每个索引文件跟踪PG中所有对象的元数据，并在存储服务器重启时加载到内存中。索引文件的每条记录包括对象ID、大小、磁盘ID、校验和和对象的分区块的位置。对象的平均元数据大小约为40字节。

**Put and Get**

提供了HTTP接口

put

> 先放入基于复制的对象存储系统，我们使用后台进程将这些对象从基于复制的存储系统导出到基于擦码的存储系统中。批量导出，避免校验块的更新开销

get

> HTTP服务器从PG对应的存储服务器获取索引。如果是降级的读请求(相应的磁盘离线或正常读期间发生错误)，HTTP服务器将从storage  Servers收集必要的数据，重新生成并将该对象传输到客户端(通过管道)。否则，对象将被直接传输到客户端(也是管道)。

几何分区要求从多个位置上读取数据，但是普通读取的性能损失很小，因为块足够大。

**IO Scheduling**

每个storage Server都有一个绑定到多个线程的FIFO请求队列，这些线程连续地从该队列提取数据。为了处理请求，存储服务器将读取所有相关数据到256KB(或更小)包，并将它们推到HTTP Server。对storage  server的后台请求(恢复、数据导入等)被放到单独的队列中，并以较低的优先级处理

**Paralleled Recovery**

 recover a node

不以对象的粒度恢复，以块恢复

单个对象的块可以并发恢复

directory Server

> 全局队列
>
> > 管理所有正在等待恢复的块
>
> HTTP Server不断地从directory Server提取这些任务,并同时恢复，恢复任务按照大小加权

### 5.2 Implementation

**Encoding Optimization**

进行了专门的优化，使用SIMD指令和循环展开等标准优化来提高性能，利用软件预取、流写入指令来减少内存访问的开销

实现了22.3GB/s的编码，18.5GB/s的解码，以及5.0GB/s的在12核服务器上使用多线程重新生成Clay(10,4)代码

**Memory Management**

一个HTTP Server管理修复块的内存池，只允许分配小于或等于256MB的块

**Range Access Support**

支持范围访问，下载大对象的一部分

## 6.Evaluation

Hardware Setup

> 16 servers
>
> dual Intel Xeon E5 2643 v4 CPU
>
> 128GB 2133 MHZ DDR4 RAM
>
> 512GB SATA3 SSD
>
> 6×8TB 7200rpm SAS HDD
>
> 56Gbps Infiniband network

所有服务器同时充当HTTP Server和Storage Server。我们选择三个服务器作为目录服务器

XFS用作底层文件系统，实验开始时，清除所有文件系统缓存

每台机器最多8个客户端，每个客户端网络带宽1Gbps

**workload**

![image-20211121164810619](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211121164810619.png)

**Parameter Settings**

(10,4) for Clay code vs  RS code VS (10,2,2) for LRC code VS Stripe-Max

ECPipe

weight 512

Hitchhiker code

**Implementation Optimizations for Baselines**

对于Stripe/RS/LRC的恢复，不是逐条恢复整个磁盘，而是将I/O请求批处理到4MB的数据块(我们发现4MB的块大小可以使性能最大化)

### 6.2 Evaluation on Production Trace

**Methodology**

关闭一个磁盘，手动启动恢复，测量恢复的时间来评估效率，同时恢复所有的PG，来衡量系统的最大恢复量

评估恢复前，评估降级读时间

![image-20211121170822055](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211121170822055.png)

![image-20211121171012775](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211121171012775.png)





## 7. Related Work

再生码在实际存储系统中的实现

> 过去专注于再生码应用在系统
>
> 没有考虑降低的读时间和流水线
>
> Vajha发现了读放大现象

Hybrid Strategies

> Panasas针对不同的对象采用不同的RAID方案。
>
> Giza主张对大于4MB的对象使用纠删码存储
>
> autoaid混合擦除编码和复制，以提高性能
>
> 这些方法关注传统的RS码，不能解决再生码的挑战
>
> Xia et.al.给i出对热数据使用更快的恢复和更低存储效率的代码，对冷数据使用更高的存储效率和更慢的恢复速度的compact code
>
> **但是clay code这样的MSR在存储效率和修复效率都是最优的，所以混合不再是必要的**

Geometric Sequences

> Buddy利用几何序列来支持动态内存分配
>
> Dartmouth使用buddy allocator来管理其文件系统中的磁盘空间
>
> Geometric Partitioning通过引入对象分区来区别于buddy allocator
>
> Geometric Partitioning通过分区对象来适应连续区域

Network Pipelining

> PRB和ECPipe通过消除数据采集节点的网络拥塞，减少修复时间。它们不是试图减少网络带宽消耗，而是通过在集群中更均匀地分配网络流量来减少修复时间。然而，它们是为满足加法结合性的擦除码而设计的，这对Clay code并不适用

## 8.Discussion

对于降级读，几何分区比分条布局需要读取更多数据，但是通过高效的流水线设计，它们可以被传输时间隐藏，所以读取时间比Stripe-Max要少得多。

几何分区将每个对象放到单个磁盘中，而不是k个中（容错性不会出问题么？）。如果有降级读，将只有一个降级读的几何分区，在CPU上负担小，由于读请求在更短的时间内降级，它对磁盘和网络的额外负担可以通过更快的恢复来补偿（这段话的含义）

RCStor存储不可变对象

再生码比LRC码占用的网络带宽少，恢复效率高（这不是全面优于？），但是LRC有更好的局部性，在跨数据中心EC中有用，因为减少了跨数据中心的流量。通过在再生代码上使用LRC代码，再生代码仍然可以提供帮助，提供更好的局域性和更少的网络带宽消耗（这个第一次见，回头可以看看）

## 9.Conclusion

揭示了大规模存储系统中应用再生码的实际差距，提出了几何分割，解决了读取时间降低和恢复效率之间的冲突。**用真实实际的traces验证了它**（本文的亮点）

# Clay Codes: Moulding MDS Codes to Yield an MSR Code

## 0.Abstract

为了保证可靠性，引入RS码，MDS码满足存储需求，但是不满足其他需要（我理解主要是网络），MSR码很多都是只有理论结构，clay code是MSR码，通过在任意单个MDS代码的多个堆叠层之间使用成对耦合，提供了一种简化的解码/修复结构。（设计思想，这部分还需要再看看）

clay code提供了MSR码的第一个实际实现

> 低存储开销
>
> 在三个关键参数方面同时优化:修复带宽、子分组级别和磁盘I/O
>
> 数据和奇偶校验节点的统一修复性能
>
> 支持单节点和多节点修复，同时允许更快和更有效的修复

MSR是**向量码**（一个新的概念）

本文对clay code性能进行了评估，存储开销增加1.25x，带宽减少2.9倍

## 1.Introduction

介绍了MDS码的优点，节省存储容量。介绍了缺点，修复成本高，增加带宽和修复时间。

MSR具有MDS所有优点，并且需要较小的修复带宽。

但是对于实际系统来说缺乏几个关键属性

> 计算上更加复杂
>
> 对不同类型的节点故障表现出不一致的修复特征
>
> 有限的容错（1，2）
>
> 缺乏常用RS的结构

本文提出了扩展之前理论的Clay codes

主要思想

> Clay codes are constructed by placing any MDS code in multiple layers and performing pair-wise coupling across layers.

本文实现了Clay,集成到了Ceph

存储开销为1.25x，修复网络流量降低2.9倍

## 2.Background and Preliminaries

**Erasure Code** 

> 介绍了下有限域，利用上标区分矢量与标量

**Scalar Codes**

> 每个chunk由L个字节组成，标量码，每个块中选择1字节，总共k字节，通过m种不同的方式，得到m字节，n=k+m称之为码字，重复L次，创造L个码字

**Vector Codes**

> 使用的是α > 1的有序集合
>
> 称之为superbyte
>
> 编码过程中，从k个数据块中挑选一个超字节，并且以m不同的方式线性组合这些超字节，以获得m校验超字节
>
> n=k+m叫做 (vector) codeword
>
> N=L/α
>
> α是超字节的子分组级
>
> Scalar Codes可以认为是α为1
>
> Vector Codes可以认为用一个向量码替代α标量码字
>
> 故障节点中的编码修复可以通过访问超字节中α字节的一个子集来实现
>
> 故障节点中的编码块的修复可以通过访问超字节中α字节的一个子集来实现，该子集存在于每个剩余的编码块中
>
> 减少了由于节点修复而引起的网络流量

![image-20211112190803451](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211112190803451.png)

**Sub-chunking through Interleaving**

> 上图与超字节相关联的α字节被连续存储，当sub-packetization level α比较大时，考虑到涉及多个码字的操作时并行执行的，从易于记忆访问的观点，交叉字节是有利的，以便不同码字的对应字节被连续存储

![image-20211112193942508](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211112193942508.png)

> 当一个块中的超字节数很大时，通过交叉，每个数据块被划分为α子集，称之为子块，因此，节点内的每个子块从存储在节点内的N个码字中的每个字持有一个字节。（这部分不太理解）

**MDS Codes**

无论时矢量还是标量，（n,k）都可以从任何n-k节点恢复，称之为MDS，有最小的冗余开销。

>  RS
>
> Row-Diagonal Parity 
>
> EVENODD 

**Node Repair**

节点修复操作，产生了大量的网络流量，RS码，流量消耗大

**MSR Codes**

Vector MDS的一种，具有最小的可能修复带宽

介绍了一些相关的参数

除了MSR低存储开销和低带宽开销的特点，还希望具有以下特点

>uniform-repair capability
>
>minimal disk read
>
>low value of sub-packetization parameter α
>
>a small size of underlying finite field over which the code is constructed

### 2.1 Related Work

节点有效修复工作

> Locally repairable codes
>
> Xorbas
>
> piggy-backed RS codes

都没达到MSR的属性

目前各种MSR都没达到所有想要的性能

![image-20211112201409926](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211112201409926.png)

FMSR codes

> 允许有效的修复，但是重构数据的功能不一定与故障数据相同，需要执行额外的解码操作来检索原始数据

Butterfly

> 通过实验验证了减少数据下载对节点修复的理论证明好处
>
> sub-packetization比较大
>
> m=2，对修复带宽限定较大

HashTag

> α≤(n−k)<sup>k/n−k</sup>
>
> 允许以修复带宽为代价灵活的选择α
>
> 仅支持系统节点的有效修复，需要在辅助节点上进行计算，并且涉及大的有限域运算

product-matrixMSR

> 具有非常低的子分组
>
> 很小的有限域大小
>
> 但是需要很大的存储开销

zig-zag codes

> 提出了第一个理论结构
>
> 当d=n-1 时，每个n，k的低存储开销的MSR码
>
> 构造是**非显式**的，决定parities的有限域系数必须由计算机搜索找到

尽管有许多理论构造和较少的实际实现，但寻找具有上述所有理想性质和实际评估的MSR代码仍然是难以实现的

 Ye and Barg的理论，改变了之前的情况

> 提供了一种结构，允许存储开销尽可能接近于1，子分组水平接近于可能的最小值，有限域大小不大于n，最佳磁盘I/O和全节点最优修复

Clay code实现了上述理论的结构，并且还具有一些额外的优势。

### 2.2 Refinements over Ye-Barg Code

Clay code

> 从耦合层的角度实现
>
> 数据解码算法和节点修复算法，都可以用两种操作
>
> > scalar MDS code
> >
> > 字节对之间的基本线性变换（Ye-Barg Code隐式，Clay Code显式）

Clay使用任意MDS，Ye-Barg code基于Vandermonde-RS codes

在耦合层架构中使用相同的MDS代码，并获得MSR的额外好处，Clay Code提出了一种用于修复多个故障的通用算法，允许我们在减少修复带宽的情况下修复多个节点。我们的改进针对于实现。

## 3. Construction of the Clay Code

**Single Codeword Description** 

> N=1,L=α

**Parameters of Clay Codes Evaluated**

![image-20211114154503959](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114154503959.png)

通过增加d-k+1，减小修复带宽

例子（n=4,k=2） d = 3, α = 4,β=2,M=8 

存储开销n/k=2,修复带宽降到了0.75

**Starting Point**

> (4,2)Scalar RS Code

![image-20211114160815149](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114160815149.png)

有相同的y，称之为由相同的y-coordinate

**The Uncoupled Code**

存储在相同的4个节点上，相同的RS码，4个码字

![image-20211114161908839](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114161908839.png)

每个节点存储4个字节，每个字节与一个不同的码字相关联，用z来索引

由4个垂直的列组成，每个列由4个柱面组成，每一个列存储一个超字节，每个柱面存储一个字节

**Using a Pair of Coordinates to Represent a Layer**

层的耦合用层的z索引来表示 z=2z<sub>0</sub>+z<sub>1</sub>

用x=z<sub>y</sub>

![image-20211114163713876](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114163713876.png)

**Pairing of V ertices and Bytes**

用p来代替（x,y,z）,p和p*耦合

被染成红色的是未配对的，剩余的顶点成对，有相同的y

16个顶点中，由8个是未配对的

图7中黄色线连接起来是配对的

![image-20211114165237884](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114165237884.png)

**Transforming from Uncoupled to Coupled-Layer Code**

![image-20211114183651181](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114183651181.png)

**Encoding the Clay code**

具体过程再看看

**Intersection Score**(IS)

## 4. Ceph and Vector MDS Codes

### 4.1 Introduction to Ceph

Ceph

> 分布式存储系统，将数据存储为对象
>
> OSD
>
> > 守护进程，与存储单元(如固态或硬盘)相关联，用于存储用户数据
>
> 支持多个EC，对象存储在逻辑分区被称之为pool
>
> 每个pool有多个PG，一个PG是n个OSDs,n是与池关联的纠删码块长度

OSDs分配是动态的，由CRUSH算法执行。

> 当一个对象流到Ceph时，分配一个PG，每当有新对象添加时，或者active osds失效时，会进行动态负载均衡
>
> 有一个OSD被指定为主OSD（p-OSD）,需要存储一个对象时，该对象被传递给已分配PG的p-OSD，该p-OSD还负责发起编码和恢复操作

从data object到数据块的传递时两步完成的。对于大对象，执行编码和解码操作所需的缓冲内存量会很大。先被划分为称之为stripes，大小用S表示，不够补0

### 4.2 Sub-Chunking through Interleaving

补0，确保S可以被kα整除

编码相当于码字为N=s/kα,下一步，从每个OSD的末尾获得α子块，每个N字节，我们使用之前L的定义，L=S/k

向量码的优点是，通过传递α子块的子集来修复被擦除的编码块，但是会导致分段读取。

### 4.3 Implementation in Ceph

实现了Jerasure和GF-Complete 

实现中，多了一块额外的缓冲区，U-buffer，用来存储U，大小为nL=Sn/k

**Pairwise Transforms**

给了sure_matrix_dotprod()和galois_w08_region_multiply()来实现{U,U<sup>∗</sup>,C,C<sup>∗</sup>},剩下两个块求解另外两个

**Encoding**

对一个对象的编码，通过p-OSD,假设m个校验块已经被erased

然后通过初始化码的解码算法，使用这些数据块恢复m个块。

与MDS编码相比，对正向和反向转换是Clay编码所需要的唯一额外计算

**Enabling Selection Between Repair & Decoding**

当多个OSDs出故障，会影响多个PG，触发所有相关对象的恢复操作，引入is_repair()，以便在带宽、磁盘I/O高效修复算法和默认解码算法之间进行选择

**Helper-Chunk Identification**

mini-mum_to_repair()，选择d个helper

**Fractional Read**

对于有效的修复，我们只读取块的一部分，通过ECSubRead

**Decode and Repair**

### 4.4 Contributions to Ceph

**Enabling vector codes in Ceph**

增加了向量码的插件

**Clay codes in Ceph**

实现了Clay Code

## 5.Experiments and Results

单节点的性能

### 5.1 Overview and Setup

我们对选定的参数集和实际比较相近的实验评价

![image-20211114204013206](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114204013206.png)

C1 VS RDP

C2 VS LRC

C3 VS RS（used in Backblaze ）

C4 C5 C6和（14，10）RS对比，主要是multiple-erasure的情况

**Experimental Setup**

M4.xlarge

> (16GB RAM, 4 CPU cores)
>
> 500G ssd

Ceph storage cluster

> consists of 26 nodes
>
> One server
>
> > dedicated for the MON daemon
>
> remaining 25 nodes
>
> > each run one OSD
>
> 总存储12.2TB

**Overview**

![image-20211114205213813](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114205213813.png)

两种负载，固定和可变

可变

> 64MB
>
> > 82.5%
>
> 32MB
>
> >  10% 
>
> 1MB
>
> > 7.5%

故障域是一个节点，我们通过移除OSDs将节点故障注入系统。

利用nmon和NMONVisualizer工具进行测量

单个PG和多个PG的方案

测量

> repair network traffic
>
> repair disk read
>
> repair time
>
> encoding time
>
>  I/O performance for degraded

### 5.2 Evaluations

**Network Traffic: Single Node Failure**

![image-20211114211057191](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114211057191.png)

![image-20211114211105064](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114211105064.png)

![image-20211114211316617](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114211316617.png)

**Disk Read: Single Node Failure**

非连续读会引起性能下降

最坏和最好的情况分析

![image-20211114211448822](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114211448822.png)

![image-20211114212023158](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211114212023158.png)









## 6 Handling Failure of Multiple Nodes



### 6.1 Evaluation of Multiple Erasures

这部分结合附录来看



## 7. Conclusions

Clay code理论上在MDS中由最小可能的修复带宽和磁盘I/O。在MSR码类中，Clay code具有最小可能水平的子分组。我们通过实验验证了这些特性的存在。

Clay code的构造分为两步

> stacks in layers, 从MDS代码中提取的α码字
>
> 将不同层的元素配对并转换以生成Clay代码

实现了从理论到实践的飞跃

# Optimizing Cauchy Reed-Solomon Codes for Fault-Tolerant Storage Applications

## 0.Abstract

对之前的柯西矩阵生成进行了改进，改进幅度高达83%，与测试的所有情况相比，平均改进幅度约为10%（为什么有部分降低了）

## 1. Introduction



# Boosting Full-Node Repair in Erasure-Coded Storage

## 0.Abstract

RepairBoost

> 调度框架
>
> 提高全节点修复性能
>
> 1. 修复抽象
> 2. 修复流量平衡
> 3. 传输调度

提高35%-97.1%

## 1.Introduction

减少修复过程中I/O放大的问题方法

> 新的编码理论
>
> 高效的修复算法，修复过程并行化
>
> 利用机器学习的预测技术，故障发生前，利用修复算法主动修复数据

现有关注单块修复，全节点修复必须同时操作多个块的修复。

存在差距导致了一些问题

> 没有用全双工运输来饱和可用带宽
>
> 没有仔细安排块的传输来充分利用带宽
>
> 忽略了不同修复算法间的弹性合作
>
> 增加了实现的复杂性

如何无缝地部署现有的修复方法以有效地解决全节点修复问题，是擦除编码存储中一个具有挑战性而又至关重要的问题

通过RepairBoost来弥补这个差距，是一个调度框架，主要思想是通过修复有向无环图（RDAG），来形成一个单块修复，描述了网络上数据路由以及需要修复的请求块之间的依赖关系。

将RDAG分解为多个修复任务每个任务执行数据上传和下载，以促进修复

平衡全节点修复中的修复流量和带宽利用率

> 将多个RDAGs的修复任务小心地分配到相应的节点上，平衡整体的上传下载修复流量
>
> 协调修复任务的执行顺序，使可用上传和下载带宽的利用率达到饱和

## 2.Background

### 2.1Basics

介绍了纠删码的概念

本文讨论RS（k+m）也适用于其他编码

### 2.2 Repair

修复

> full-node repair
>
> degraded reads

本文关注full-node repair

修复代价比较高，研究减小修复代价

> Repair-efficient erasure codes
>
> > LRC和再生码
>
> Repair algorithms
>
> > 利用未占用的带宽来加速修复

![image-20211201194236314](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211201194236314.png)

举了个例子，减小修复时间段

> Proactive repair with erasure coding
>
> 有效修复的纠删码和修复算法，发生故障后才启动修复工作
>
> 可以通过机器学习预测，提前预测到要发生故障的节点，提前修复，本文主要研究后一种办法

### 2.3 Limitations

**Limitation 1 (L1): Failing to utilize the full duplex transmission**

> 上传和下载是独立的
>
> 图3

![image-20211201200411278](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211201200411278.png) 

**Limitation 2 (L2): Failing to fully utilize the bandwidth at each timeslot.**

> 现有的修复算法可以在单个块修复中缓解下载瓶颈
>
> 全节点修复可能无意中再次导致链路拥塞
>
> 图4

![image-20211201202552562](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211201202552562.png)

**Limitation 3 (L3): Inflexibility**

> 很多修复算法平等地对待每个块，并使用相同地修复算法修复所有的块
>
> 没有考虑到不同的可靠性要求和热度倾斜问题

**Limitation 4 (L4): Lack of a general framework for the full-node repair **

没有通用的框架，同时支持不同类型的修复算法

## 3.RepairBoost Design

提高全节点修复性能

**Assumptions**

首先关注单节点故障，然后扩展到多节点故障

**Overview**

通过RDAG抽象出一个单块修复方案，通过支持多个RDAG的调度，实现灵活性，支持不同修复算法的协作，通用性（L3 和L4的情况）

### 3.1 Repair Abstraction

**RDAG construction**

抽象出了一个RDAG图

**Repair process guided by RDAG**

从叶子节点开始修复

**Example**

图五

![image-20211202110940232](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211202110940232.png)

**Advantages of an RDAG**

使用这个图的好处

适用于各种编码方案

描述了各种依赖关系

表示了每个顶点的修复任务

**Discussion**

RDAG和ECDAG的区别

### 3.2 Repair Traffic Balancing

整个系统的上传和下载流量应尽可能均衡

**Retaining fault tolerance degree**

保证容错

**Balancing repair traffic**

三种节点

> 叶子节点
>
> 根节点
>
> 中间节点

先映射中间节点和根节点，平衡下载流量

最后分配叶子节点，平衡上传流量

![image-20211202141342816](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211202141342816.png)

### 3.3 Transmission Scheduling

平衡整体上传和下载修复流量后，不一定能达到修复时间的下限。因为在修复期间，带宽可能不会在每个时间段被利用

最大流问题，图7

![image-20211202143138002](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211202143138002.png)

时间复杂度O(n<sup>2</sup>e)

### 3.4 Extensions

**Multi-node repair**

两种选择

> 逐个修复每个失败的节点
>
> 优先修复包含较多故障块或较多访问块的条带

**Heterogeneous environments**

可以根据环境改变

在轮询模式下采用以下方式进行传输调度

> 定位上传和下载数据时间最短的节点
>
> 根据可用带宽 进行排序
>
> 从拥有最大上传或下载带宽的链接中选择数据传输

**Adaptation to network conditions**

可以适应网络条件

## 4. Implementation

**System architecture**

图8

![image-20211202153454215](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211202153454215.png)

> coordinator 
>
> > 位于元数据服务器
>
> agents
>
> > 存在节点上

**Operating flow**

向元数据服务器报告故障事件，协调块确定丢失块的id以及关联条带的身份，选择修复方案，发送给代理

代理收到后

> 读取本地存储的幸存块请求
>
> 将他们发送到指定的中继节点
>
> 解码（修复）丢失的块

repairboost将一个块分割成许多更小的包，使用多线程，做pipeline

**Integration with Hadoop HDFS**

在NameNode上部署coordinator

在DataNode上部署agent

## 5.Performance Evaluation

实验总结

> 吞吐量提高35.0-97.1%
>
> 可以协调多种EC
>
> 在低网络带宽的环境中更有优势
>
> 在异构和多故障修复种仍能保持其有效性

### 5.1Setup

> EC2
>
> 17 virtual machine instances 
>
> m5.large
>
> Ubuntu 16.04.7 LTS
>
>  two vCPUs with 2.5 GHz Intel Xeon Platinum
>
> 8 GB RAM
>
> 40 GB of EBS storage

先写数据，warm up

用擦除来模拟节点故障

从报告故障事件到修复和持久化所有丢失数据的时延

修复吞吐量

较高的修复吞吐量意味着较短的漏洞窗口

**Erasure codes and repair algorithms**

RS,LRC,MSR

CR,PPR,ECPipe

**Selection of baseline**

random selection(RAN),LRU-based selection approach

**Default configurations**

chunk size 64MB

packet 1MB

### 5.2 Experiments on Sensitivity

图9实验结果

![image-20211202161817066](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211202161817066.png)

### 5.3 Experiments onRepairBoostProperty



![image-20211202163539223](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211202163539223.png)

![image-20211202163743957](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211202163743957.png)

### 5.4 Experiments on Practicality

![image-20211202164401176](C:\Users\nty\AppData\Roaming\Typora\typora-user-images\image-20211202164401176.png)







## 6.Related Work

**Repair-efficient codes**

**Repair algorithms**

**Proactive repair with erasure coding**



## 7.Conclusion

RepairBoost

> 一个调度框架

# Crash Consistent Non-Volatile Memory Express

## 0.Abstract



# Fast Predictive Repair in Erasure-Coded Storage

## 0.Abstract

EC提供存储效率高的冗余机制，可以预测到提前即将故障(STF)节点，提供了新的机会，提出了一种快速预测方案FastPR，讲迁移和重构两种修复方法结合在一起，使修复操作在整个存储集群中完全并行化，解决了二部图的最大匹配问题，显著减少了基线修复方法的修复时间

## I. INTRODUCTION















